\documentclass[sigplan,10pt]{acmart}
%\documentclass{article}
\usepackage{graphicx}
\usepackage[caption=false]{subfig}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Expos\'{e}: Leveraging Mathematical Structure in Software Synthesis}
\author{Andr\'{e}s Goens}
\begin{document}
\date{}
\begin{abstract}
The trend is clear: Computing systems of the future will consist of heterogeneous multicores.
While this allows for maximal computational efficiency, these systems are extremely difficult to program.
A family of methods that is commonly used to program heterogeneous multicore systems is called \emph{software synthesis}.
It consists of using an abstract representation of the application in order to analyze it and determine an optimal execution, leveraging heterogeneous resources.
The software synthesis problem has many different subproblems, pertaining the representation of applications and architectures, recording the behavior of an execution and deciding how to best execute a particular application in an architecture.
Each of these subproblems exhibits much structure, which can be captured in precise mathematical terms. In this thesis we aim to identify, describe and leverage this mathematical structure in the different subproblems of the software synthesis process.
This allows to improve the programmability of heterogeneous multicores, bridging the productivity gap for developers.

\end{abstract}

\maketitle
%\tableofcontents

\section*{Introduction}
%End of moore's law:
Since the invention of the semiconductor, the capacity and performance of computing systems has enjoyed a nearly exponential growth over time.
Historically, it has doubled roughly every $1~1/2$ years, an observation which is commonly associated with Moore \cite{schaller1997moore}.
In terms of the number of semiconductors in a computer this trend continues roughly until today. However, the capacity and speed of computing systems has been unable to keep up.
It is likely that we are reaching more fundamental barriers, where the laws of physics prevent our current methods to produce event faster systems. 

%Trend: heterogeneous multicores
Fortunately, we still seem far from reaching the limits of our own ingenuity. In terms of hardware, the trend is clear: The computing systems of the future will consist of heterogeneous multicores. Multicores allow to leverage the technological advances in semiconductors to 
increase the computing power of systems by exploiting parallelism in computation. The heterogeneity of said systems, on the other hand, allows to design units specialized for a particular type of processing, instead of relying solely on general-purpose processors.
While this specialization can have a tremendous impact in the computation speed for some problems, it usually has an even greater impact on energy efficiency, which is arguably even more valuable today.


%problem: programming heterogeneous multicores?
With novel systems come novel problems. While heterogeneous multicores are, in principle, an ideal solution to the complex requirements of today's applications, they come with an equally exceptional challenge.
Programming heterogeneous multicores is difficult. It involves partitioning the required computation into appropriate steps. These steps, in turn, might execute differently well in the various hardware resources on the system,
which might itself depend on the concrete data being processed.
On the other hand, systems incur in significant time and energy costs by transfering data between computing resources.
To efficiently execute a calculation, thus, involves orchestrating which steps to execute when and where.
This is a highly non-trivial problem which completely dissapears in the traditional, single-core case.

%a solution: software synthesis
Several approaches have been put forward to deal with these issues.
In particular, a family of promising methods that is commonly used to program heterogeneous multicore systems is called \emph{software synthesis}\cite{bhattacharyya2012software,sgroi1999synthesis}.
It consists of using an abstract representation to describe the application, commonly in the form of a graph.
Using this representation, decisions for mapping the different parts of the application to resources in the architecture, as well as scheduling the execution are taken.
These decisions commonly involve static strategies, but can also include dynamic decisions at run-time.
Finally, code is generated using these decisions, such that it might leverage the heterogeneous resources in the architecture.

%many sub-problems in software synthesis: code partitioning/representation of algorithms, tracing/trace analysis, mapping/scheduling, code generation
This software synthesis process, thus, involves several different sub-problems that have to be solved.
The question of how to partition an application and represent it abstractly, particularly, in form of a graph that represents logical dependencies in the application, is a highly non-trivial one.
Assuming such a graph representation, the behavior of the application has to be analyzed.
A general analysis would require, among others, solving the halting problem, which is provably unsolvable.
Thus, many common approaches make do with a trace-based approach, where the analysis is restricted to a particular execution, or a set of executions.
Finally, the decision problems of mapping and scheduling involve the knowledge gained from the sub-problems above to consider an exponentially growing number of possibilities and selecting a near-optimal one.

%Problems have mathematical structure! Graph structure of code, traces: monoid, redundancy,  mapping/scheduling (e.g. symmetries, distances)
By virtue of their inherent properties, these problems exhibit a large amount of structure, which can be described and exploited in precise mathematical terms.
The simplest example is the abstract representation of applications.
Using the mathematical structure of directed graphs, properties of the application can be deduced from graph-theoretic principles and graph algorithms can be leveraged for efficiently analyzing these abstract representations of applications.
Similarly, the sets of execution traces of an application can be considered as part of a monoid, usually called the trace monoid. They also have information-theoretic properties that correspond to the relationship between the application and its trace.
The space of mappings, on the other hand, while it grows exponentially, also exhibits much structure.
For example, symmetries in the architecture and application induce symmetries in the mapping space, which can effectively reduce the amount of possibilities to be explored.

%thesis: identify and leverage this mathematical structure to improve software synthesis!
In order to feasibly solve the problems arising in software synthesis and program the heterogeneous manycores with their ever-growing complexity, we must identify and exploit these mathematical structures.
The goal of this thesis is to identify and precisely describe the mathematical structure in the different subproblems in software synthesis.
Then, using the identified structure, we will propose improvements to the state-of-the-art solution methods in order to leverage the mathematical structure inherent in the problems.
In this exposé we briefly introduce the software synthesis problem with its rich structure (Section~\ref{sec:software_synthesis}), present a selection of previous work analysing and leveraging this structure (Section~\ref{sec:current}), as well as some concrete research directions to improve this (Section~\ref{sec:future}.
%Particular for berkely
In particular, we present a proposal for a research stay at Prof. Edward Lee's group in the context of this work and argue why it would be extremely beneficial for the success of the thesis (Section~\ref{sec:visit}).

\section{The Software Synthesis Problem}
\label{sec:software_synthesis}

%General software synthesis too broad: concentrate
The term software synthesis covers several different concrete problems, with different abstract representations of applications or architectures.
The aim of this thesis is to keep the approaches as general as possible. However, to cover as many variants as possible, concrete instances still have to be considered.
In the following, we will describe the software synthesis problem for executing Kahn Process Networks~\cite{kahn74} in heterogeneous architectures, as described in~\cite{castrillon14_springer}.
While this is not the most general instance of software synthesis, it provides enough generality to understand most concepts, while serving as a concrete realization that allows to implement and test the improvements on real examples.
%Throughout the thesis we will use this instance as the basis for the software synthesis problem, and will explicitly discuss, when more generality is possible, how this is the case and how to leverage it.

\subsection{Software Synthesis for Kahn Process Networks}

The problem of mapping KPN applications to heterogeneous hardware involves abstractions at different levels, as depicted in Figure~\ref{fig:problem}.
At the \textbf{application} level, programs are described using the KPN model. In this model, applications are partitioned into
different \emph{processes}, which encapsulate the different parts of the computation. These processes are not isolated;
they communicate by exchanging data. In the KPN model this is abstracted by defining communication \emph{channels}
between processes that act as unbounded FIFO buffers. The Kahn \emph{process network} is the resulting system, usually formalized as a graph 
with processes as nodes and channels as edges.

 \begin{figure}
 	\centering
 	\includegraphics[width=.48\textwidth]{figures/problem.pdf} 
 	%	\vspace{-5mm}
 	\caption{An instance of software synthesis: Mapping KPN applications to heterogeneous architectures.}
 	\label{fig:problem}
 		\vspace{-3mm}
 \end{figure}

The structure offered by KPNs has several advantages. One of these comes at the \textbf{execution} level. 
From its semantics, KPN applications are deterministic~\cite{kahn74}. 
This means that the process network will always produce the same results given the same input.
In particular, the results are independent of the execution order of the processes and individual timings,
provided no artificial deadlocks are introduced when restricting the sizes of the FIFO buffers.
This fact can be utilized to create program traces of a KPN application that are independent of the execution,
capturing the behavior of the execution at a high abstraction level.

Abstractions are also required for describing the hardware \textbf{architecture}.
To this end, an architecture is described as a set of \emph{processing elements} (PEs) and \emph{communication resources}.
The latter is an abstraction for any way data can be shared between processes residing on one or several PEs. 
These range from simply shared memories, and local scratchpads for single PEs, to specialized resources like hardware-FIFOs.
Actual hardware architectures, and the libraries used (or operating system, if applicable), are much more complex than this.
However, there is no simple abstraction that allows to capture all these details in a straightforward manner, 
which is why different frameworks use different such abstractions~\cite{goens_mcsoc16}.

Using these models, \textbf{decisions} are made for deploying the applications onto the hardware.
These decisions involve abstractions.
As is canonical from the application description, processes are mapped to PEs, 
and the communication channels between them to hardware resources like memories.
It is also common to distinguish between the mapping of processes to PEs and the scheduling at runtime.
In this discussion, we limit scheduling to a single PE when several processes have been mapped to it.
There are additional decisions that have to be taken in this context, like \emph{buffer sizing}, where the sizes of the
FIFO buffers have to be chosen or selecting different voltage and frequency domains in case these can be set within the chip. 

The problem of mapping KPN applications to heterogenous hardware, as studied in this thesis, is that of
finding a mapping of processes to PEs and of channels to communication resources, and may or may not include the additional decisions mentioned above.
A particular objective or set of objectives is specified.
The list of possible objectives is varied, spanning, for example, the total execution time, energy consumption or resource usage (while respecting a particular real-time bound).
With regards to these objectives, the mapping should be optimal or at least near-optimal for a particular execution, which is captured in the form of a trace.

A concrete implementation aimed at solving this precise problem is realized in the commercial software suite SLX, which is based on the MAPS framework~\cite{maps08,castrillon14_springer}
Applications are written as KPNs in an extension to the C programming language, CPN~\cite{sheng2014_cpn}. Similarly, architecture descriptions are given as XML files.
The tool uses novel methods for performance estimation to extrapolate the performance of the execution in the target architecture~\cite{TRETS_eusse14}.
It then generates a mapping with heuristics~\cite{castrillon2012,castrillon_industrial_informatics} and uses it to produce executable code for the target.
Other similar approaches have been also put forward, like the Sesame framework~\ref{sesame}, or DOL~\cite{thiele2007mapping}.
%This thesis will use SLX tools to evaluate approaches and improvements, whenever applicable.

Figure~\ref{fig:synthesisflow} shows an abstract representation of the software synthesis flow, as described above and implemented in SLX.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.45\textwidth]{figures/synthesisflow.pdf}
	\caption{Abstract view of a software synthesis flow}
	\label{fig:synthesisflow}
\end{figure}


\section{Current Status}
\label{sec:status}

In the following section we will describe our previous work leveraging mathematical structure in software synthesis.
For space reasons, this exposé cannot go in-depth in the mathematical structures and the methods devised to leverage them.
Similarly, for space reasons as well, not all our previous work can be explained in the same depth.
Instead, the aim of this section is to give an overview and present a selection of results.
They show how, in principle, mathematical structure can be leveraged to improve solutions to the software synthesis problem.

\subsection{Symmetry in mappings}\label{sec:symmetries}

In practice, the mapping of applications described by an abstract, graph-based representation to heterogeneous architectures exhibits many symmetries~\cite{goens_iess15,goens_taco17}.
Consider the example shown in Figure~\ref{fig:symmintro}. To illustrate the principle, the figure depicts a simple homogeneous many-core with a network-on-chip interconnect.
Due to the homogeneous structures of the platform, it is intuitive to see that the two leftmost mappings of tasks to processing nodes should lead to basically the same execution behavior, if we neglect effects like process variation or aging.
Conversely, we can expect the third, rightmost mapping to have a different execution behavior, since the communication paths are different than in the first two. 
In the presence of heterogeneous resources, such an analysis becomes less obvious.
Even more so with complex network topologies, in architectures with hierarchical structure, when optimizing for different objectives, or when all these are combined.
To effectively describe and leverage these symmetries thus, we need a structured approach that leverages this structure.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.45\textwidth]{figures/SymmetriesIntro.pdf}
	\caption{Illustration of symmetries. The two left mappings are equivalent, while the one on the right is not.}
	\label{fig:symmintro}
\end{figure}


\subsubsection{Mathematical Structures}

In order to describe mappings of KPNs to heterogeneous architectures mathematically, we need a mathematical description of the architecture and of the KPN. 
A KPN has a very precise mathematical definition of its semantics, as can be found in the original paper by Gilles Kahn~\cite{kahn74}.
However, for this exposé the semantics are not essential, although we address it in other work (currently under submission).
Thus, it suffices to consider a KPN application as a directed graph $K = (V_K,E_K)$ with a vertex set $V_K$ of processes, and an edge set $E_K$ of channels.
%Processes can execute concurrently and communicate solely through the channels, which can be thought of as FIFO buffers.

For describing architectures, our approach uses a structure called the \emph{architecture graph} to capture the architecture topology~\cite{castrillon2012}.
This labeled multigraph $A = (V_A, E_A)$ has a node $v \in V_A$ for every processing element (PE) in the architecture.
A function $l$ labels all the PEs in $V_A$ with their PE type in heterogeneous architectures.

For every communication resource $r$ that can be used between two PEs $v_1, v_2 \in V_A$, the architecture graph has an edge $(v_1,v_2,r) \in E_A$, where $r$ is a label for that communication resource.
As an example, consider the Exynos architecture, as illustrated in Figure~\ref{fig:exynos}.
It has eight ARM PEs, following the big.LITTLE\texttrademark~principle, with four ARM Cortex A7 PEs (the ``little'' ones) and four ARM Cortex A15 PEs (the ``big'' ones)~\cite{biglittlewhitepaper}. 
The architecture graph of the Exynos architecture is depicted in Figure~\ref{fig:arch_graph}.
The labels for the different communication resources and PE types can be seen as the colors of the edges and nodes in the graph.

This architecture graph can be readily obtained from the architecture description in the SLX Tool Suite, or any similar description which includes the structure of the hardware architecture. 
In fact, the objective of the architecture graph is to allow us to capture, in a mathematical object, precisely this \textbf{structure}: the topology, PE types and the communication resources available.
Only by using the formal nature of this architecture graph we can extract the symmetries of the architecture algorithmically, i.e., in an automated fashion.


\begin{figure*}[t!]
%
	\subfloat[The heterogeneous Exynos architecture.\label{fig:exynos}]{
     \begin{minipage}[b]{0.41\textwidth}
	\centering
	\includegraphics[width=0.8\textwidth]{figures/exynos.pdf}
	%
	%
		\end{minipage}
	}
	\hfill
	%
	\centering
	\subfloat[The architecture graph of the Exynos.\label{fig:arch_graph}]{
		\begin{minipage}[b]{0.54\textwidth}
	\centering
	\includegraphics[width=0.94\textwidth]{figures/exynos_archgraph.pdf}
	%
	%
		\end{minipage}
	}
	%
	\caption{An example architecture and its corresponding architecture graphs.}
	\label{fig:actionsm1}
 \end{figure*}
 
A mapping of a KPN $K$ to an architecture with architecture graph $A$ is simply a morphism of graphs $m: K \rightarrow A$.
This means that edges from $K$ are mapped to edges from $A$ and ensures that FIFO buffers can be realized.
In practice, FIFO buffers and communication resources have an annotation to their size, and a valid mapping would has to respect these sizing constraints.
In this exposé, however, we will disregard these issues for simplicity.

An automorphism $\varphi: A \rightarrow A$ of the architecture graph, i.e. a graph isomorphism respecting labels of $A$ to itself, describes a symmetry of the problem.
Since even heterogenous architectures usually have several identical components, the groups of automorphisms of architecture graphs $A$ are non-trivial~\cite{goens_iess15}.
These symmetries of the architecture induce symmetries on mappings, as illustarted in Figure~\ref{fig:symmintro}.
Similarly, if we know an automorphism from the semantics, it also induces symmetries in the space of mappings. 

In general, however, to capture all symmetries of an architecture that can be exploited, the approach using group theory is not sufficient. 
Consider the NoC-mesh as depicted in Figure~\ref{fig:4x4NoC_inv_semi}.
Compare the colored regions in the different depictions of the architecture in the figure.
A computation using only the cores of the blue region to the left should have the same performance values as the one in the blue region to the right, with the transformation implied by the dark-blue arrow:
\begin{align}
\label{eq:partial_4by4}\text{PE}_1 \mapsto \text{PE}_{2}, \text{PE}_{5} \mapsto \text{PE}_{6}, \text{PE}_9 \mapsto \text{PE}_{10}, \text{PE}_{13} \mapsto \text{PE}_{14}
\end{align}
However, this mapping is not an automorphism of the architecture graph corresponding to this mesh.
The fundamental difference between this transformation and symmetries of the automorphism group, like rotations and reflections is that this transformation is only defined on a subset of the four-by-four mesh.
It cannot be extended to a symmetry of the full mesh.
To accurately describe and leverage symmetries like that depicted in Figure~\ref{fig:4x4NoC_inv_semi}, an approach using the mathematical theory of inverse semigroups is required~\cite{goens_taco17}.
For space reasons, we will not delve into the details of this approach.
Instead, we will present two applications of the described symmetries to improve software synthesis.

\begin{figure}[h]
	\centering
	\includegraphics[width=.46\textwidth]{figures/NoC4by4inv_semi.pdf}
	%
	\caption{Partial functions define more symmetries in a 4x4 mesh.}
	\label{fig:4x4NoC_inv_semi}
		%
\end{figure}


%Inverse semigroups

\subsubsection{Reducing design-space exploration}

An application of the symmetries found using the graph formalizations and the theories of groups and inverse semigroups is a reduction of the design-space of mappings. 
To find a good mapping of an application to an architecture, a designer or tool doing software synthesis has to explore an extensive design-space of possible mappings. 
If two mappings are equivalent through symmetries, however, only one of them has to be considered.

Following this principle, we modified a design space exploration framework to leverage the symmetries of the architecture.
The design-space exploration uses evolutionary algorithms within the Sesame~\cite{sesame} framework to explore the space of mappings. 
It has a cache which tests if a configuration has been evaluated before, in order to avoid reevaluation.
We augumented this cache with a symmetry-testing improvement. If an equivalent mapping has already been evaluated before, it will not re-evaluate it.
The original cache does the same, but only of if the mappings are identical.
We tested several applications applications from the signal processing and multimedia domains by mapping on an accurate model of a state-of-the art heterogeneous multicore system, the Texas Instruments (TI) Keystone II.
It features 4 ARM Cortex-A15 processing elements and 8 DSPs~\cite{keystone2_whitepaper}. 
The model was adapted~\cite{goens_mcsoc16} from a model of the commercial state-of-art MPSoC compiler from the SLX Tool Suite and confirmed with measurements from hardware~\cite{odendahl2013split}.

\begin{figure*}
	\centering
	\includegraphics[width=0.9\textwidth]{figures/wall_genetic.pdf}
	%
	\caption{A comparison of benchmarks for the improved caching strategy in Sesame. The red bars on top represent the overhead, while the dotted gray line marks the baseline (without symmetries). }
	\label{fig:genetic_algorithms}
	%
\end{figure*}

We executed the DSE framework with $5, 20, 50$ and $200$ generations for all benchmarks, with five different (fixed) random seeds, repeating each such configuration five times.
It uses an evolution-strategy called $\mu + \lambda$, which we used with fixed populations of size $20$ and with $20$ children.
We measured the overall wall-clock time of the DSE using our improved cache strategy and the conventional cache. Additionally, we
measured explicitly the time spent in symmetry-related calculations in the improved cache. The results are summarized in Figure~\ref{fig:genetic_algorithms}.
It shows the normalized average of the total execution time for all benchmarks, separating the time spent on the DSE from the overhead of the symmetry calculations. 
The difference to the baseline of $1$ is the percentage of time which was saved by avoiding redundant calculations.
The plot shows the overhead is negligible in all cases; indeed, it represents between $0.1-0.2 \%$ of the total execution time across the different benchmarks.
The plot also shows that the improved cache results in a net improvement of the execution time, in spite being a prototype implementation. 
As expected, the effects of symmetry were more visible in explorations with a larger number of generations (and thus, of simulations).

Similarly, we modified an iterative algorithm for minimal resource usage in soft real-time scenarios in SLX.
We obtained significantly better resource utilization by considering symmetries, compared to a simple algorithm which does not consider them. 
Compared to a brute-force approach, we obtained a speed-up by a factor of $10$.
More details of both usecases, as well as the theory behind the symmetry identification and reduction can be found in~\cite{goens_taco17}.

\subsubsection{Run-time adaptivity}

Another application of the structure of symmetries comes in the context of run-time adaptivity.
When designing software for embedded systems, reliability and predictability are usually just as much an asset as raw performance~\cite{edwards2007case,axer2014building,lee2008cyber}.
Today, with the increasing availability of high-performance embedded devices, like the ARM big.LITTLE\texttrademark~platforms~\cite{biglittlewhitepaper} or the many-core Epiphany-based systems~\cite{olofsson2016epiphany}, the lines between commodity, high-performance and embedded system devices and ecosystems are becoming increasingly blurred.
Several modern embedded systems interact with the environment in a plethora of ways at the same time.
This increases the unpredictability of software execution, since the workload of the system depends on many different and independent applications~\cite{lee2008cyber}.
%This is especially relevant when the system is designed to interact with the physical environment, be it a person watching a video on a tablet device, an LTE decoder on her phone
%or even the pacemaker that keeps her heart beating. For (hard) real-time systems, like the pacemaker, performance guarantees are non-negotiable and should be met at any cost.
%On most other applications, though, a reliable performance is desirable only to a reasonable cost. 
%This is understandable: a missed frame is usually not as disastrous as a missed heartbeat.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.40\textwidth]{figures/tetris.pdf}
	\caption{The basic idea behind TETRiS}
	\label{fig:tetris_idea}
%  \vspace{-3mm}
\end{figure}

In several cases, the traditional notions of real-time software do not apply anymore: many applications are too complex and variable, yet not critical enough for a rigorous worst-case execution-time analysis and verification.
Consider a modern mobile phone.
It has to keep several background processes running for the normal functioning of the phone, while being used for high-quality video playback.
If then, a high-performance request gets scheduled, e.g., from a Bluetooth device which wants to do simultaneous location and mapping using the phone, it would be ideal to keep the same performance for the other tasks, without changing the user experience.
To ensure this, the allocation of resources might need to be adapted dynamically, while keeping the performance equal.
We need approaches that ensure an execution that is predictable and efficient on average, even in the presence of dynamic changes in the system's load.

A large body of research has been dedicated to programming applications for these modern embedded systems~\cite{eker2003taming,thiele2007mapping,maps08,pimentel2006systematic,nikolov2008daedalus}.
In particular, it is a common practice to use static mapping and scheduling strategies at compile-time to obtain predictable outcomes~\cite{bekooij2004predictable}.
However, flows considering static resource allocation and concentrating on a single application struggle when dealing with multiple applications.
The estimated performance of an application does not consider other applications that will compete for system resources.
This leads to significantly worse and less predictable performance when, at run-time, this isolated view of the application does not hold anymore.
To avoid contention in the communication, hardware support is required.
This can mean Network-on-Chip technologies or more dedicated mechanisms~\cite{hansson2009compsoc,kumar2008analyzing} to ensure isolation and predictable communication patterns between tasks. 
These concepts, along the lines with the concept of precision timed (PRET) machines~\cite{edwards2007case}, require very specific hardware, not all of which has been embraced by mainstream chip manufacturers yet.

We can leverage the same mathematical structure of symmetries to deal with these issues.
Instead of considering only one mapping out of a whole equivalence class in the desgin-space exploration at compile-time, we can leverage the equivalence class at run-time.
In the following we present a novel approach to keep applications in their isolated view at the software level, obtaining reliable performance in dynamic multi-application scenarios.
While the methods should produce the best results on precision-timed machines, our implementation also works on off-the-shelf heterogeneous multi-processor system-on-chips (MPSoCs) and is compatible with Linux.
We call our approach the \textbf{T}ransitive \textbf{E}fficient \textbf{T}emplate \textbf{R}un-t\textbf{i}me \textbf{S}ystem (TETRiS).

Figure~\ref{fig:tetris_idea} summarizes the idea of the TETRiS approach.
It uses near-optimal static mappings for the isolated applications, as depicted on the upper side of Figure~\ref{fig:tetris_idea}.
To execute multiple applications with a given near-optical mapping, a strategy is required to combine the mappings in a configuration.
Figure~\ref{fig:tetris_idea} shows what would happen when using no strategy at all, which results in a very inefficient resource distribution.
It also depicts an unpredictable redistribution, as would probably be computed, e.g., by the operating system's scheduler (CFS, the Completely Fair Scheduler, in the case of Linux).
Finally, the figure depicts our approach, TETRiS, where the structure of the mapping is preserved.
By using the mathematical structure from the symmetries, it can automatically and precisely identify, at compile-time, classes of mappings that will have the same performance on a system.
Since our method leverages the structure of the architecture only, it is agnostic to the mapping strategy and its objectives.
By using a strategic precomputation step, this can be done with almost no overhead at run-time. 

We evaluate TETRiS approach in an implementation using SLX, and executing it on the Hardkernel Odroid XU3, a modern off-the-shelf heterogeneous multicore system.
The system features an Exynos~5422 big.LITTLE chip with four Cortex-A15 cores and four Cortex-A7. 
We used two different applications for benchmarks, AF, a band-pass filter for stereo files, and MIMO, a multiple-input multiple-output orthogonal frequency-division multiplexer.
For our benchmarks we measured the wall-clock time, which is the time as (humanly) perceived from start to finish.
%The energy was measured by accessing the on-board energy sensors featured on the Odroid XU3.
%These INA-231 sensors are connected via the I2C bus and measure the energy at the voltage regulators of individual components.
%They provide detailed energy data for the big cluster, little cluster, memory and GPU of the system, sampled at \unit[10]{Hz}.

 \begin{figure}
 	\centering
 	\includegraphics[width=.48\textwidth]{figures/wall_af_mimo.pdf} 
 	%	\vspace{-5mm}
 	\caption{A comparison of different strategies for executing multiple applications.}
 	\label{fig:tetris_eval}
 		\vspace{-3mm}
 \end{figure}

Figure~\ref{fig:tetris_eval} shows the comparison of different strategies for combining mappings. The first, CFS, lets the Linux scheduler freely decide the scheduling of processes to cores. 
The strategy marked with SLX M. uses mappings obtained with SLX and the TETRiS to execute equivalent mappings for executing the different applications in parallel. 
Finally, the C. Pin. strategy uses a mixture of both, by pinning applications only to the cores selected by the equivalent mappings for TETRiS, but letting Linux schedule the processes completely within the constrains of these cores.
The figure shows the box plot for $50$ executions of the experiment, where two instances of the AF benchmark run concurrently with two instances of MIMO. 
It shows how TETRiS can effectively preserve the properties of mappings and produce more reliable and predictable execution, even in an environment like Linux.
Moreover, while the average execution time of the dynamic schedulers is better, the worst case execution time in the experiments was worst for both strategies, CFS and C. Pin, compared to SLX M. 

%\subsection{Mapping Algorithms}
%The approaches we have outlined so far assume an existing algorithm for mapping and improve it. However, the mathematical structure of the problem can also be leveraged to improve 
%\subsubsection{Comparing Algorithms}
%%The need for abstract models of architecture
%%Metaheuristics > Heuristics, problem: time!
%\subsubsection{Design centering}
 %%Robust mappings

\section{Future Work}
\label{sec:future}

While previous work already illustrates how mathematical structure can be effectively leveraged and exploited for improving software synthesis, there are several additional structures in software synthesis and promising ways to further improve these methods.
In this section we propose diverse options of other mathematical structures in software synthesis that can be explored, or ways to leverage the ones identified already.

\subsection{Application Graphs}

The structure of the application as a graph is already a well-defined mathematical structure with useful properties.
Not only dataflow graphs, but also task graphs or other dependency graphs all share the property of describing the flow of the algorithm in a graph structure.
This structure can be leveraged for analysis or for transformations.

\subsubsection{Graph transformations}
In dataflow execution models, the dataflow graph is part of the program semantics. By formalizing these semantics, giving them a mathematical structure, a graph transformation can be performed such that it preserves the program semantics.
This can be used to improve an execution by doing I/O-optimizing transformations, or increasing the number of parallel executions when data-parallelism is involved.
We are currently working on \"Yauhau, an engine that transforms dataflow graphs to improve I/O in microservice-based architectures. 

\subsubsection{Graph semantics}
The KPN semantics are usually implemented using the Kahn-MacQueen execution model~\cite{kahnmacqueen}. This model uses blocking reads and writes on communication channels in order to ensure a deterministic execution.
However, the semantics as defined by Kahn~\cite{kahn74} do not require these blocking writes and reads. A model with a non-deterministic execution order, which nevertheless ensures the ordering of channel tokens is thus also compatible with the KPN semantics.
This ``MacQueen Gap'' can be thus leveraged to efficiently execute instances of data-parallelism in Kahn Process Networks.

\subsubsection{Application Symmetries}
As explained in Section~\ref{sec:symmetries}, symmetries at the application (graph) level can be leveraged for mapping just as those for the architecture.
While determining the symmetries for the architecture is straightforward from the architecture graph, the same does not hold for the application.
In order to determine if a graph automorphism of the application graph is indeed a symmetry in the execution, the semantics of the program have to remain unchanged by it. 
This could be verified using compiler technologies.

\subsection{Architectures}
Considering the target architecture or hardware platform is crucial for the software synthesis problem.
In order to effectively rely on the different, possibly heterogenous, components of the system, it is vital to know what these components are and what they are capable of.


\subsubsection{Formal models of architecture}
The architecture and the corresponding architecture graph have a precise mathematical structure, just as the application. However, the applications have well-defined models that include the whole semantics, through the programming languages used to define them.
Architectures, on the other hand, are usually modeled in an ad-hoc fashion. While the architecture graph greatly reflects structural components of the architecture, like its topology, it fails to include several aspects.
In particular, an architecture graph cannot be used to reconstruct the ad-hoc models with the level of detail required by e.g. a simulator.
A formal description of an architecture is also useful for comparing different software synthesis flows~\cite{goens_mcsoc16}.
Such models have already been proposed~\cite{moa}, and in the work of this thesis we plan to study them and leverage them, or improve them if necessary.

\subsubsection{Symmetries}
The work on symmetries, while providing a base for leveraging the symmetries of the architecture, still has much room for improvement. Especially for NoC-based architectures, the time required to calculate symmetries is too high, since general algorithms are used.
In future work we plan to leverage domain-specific knowledge about the possible (and consequently, about the impossible) instances of symmetry semigroups that can arise in software synthesis to greatly improve the algorithms in this application.

\subsection{Traces}
Modern applications are often complex and very dynamic.
Depending on the data, the execution of two instances of the same application might have a very different behavior.
Even if the behavior is more predictable, it has to be captured mathematically in order to leverage its structure.
This is done through execution traces. Traces record different events throughout the execution and annotate them in different ways. Mathematically, the Mazurkiewicz traces capture the structure of these execution logs~\cite{bookoftraces}.

\subsubsection{Multiple Traces}
In order to deal with the dynamic behavior of applications, the behavior of different executions of a single application has to be captured and analyzed.
This includes understanding those parts of the execution behavior of an application that are common to all inputs, as well as potential differences that might be input-dependent.
This can be directly translated into the language of traces.
For a deterministic model of computation, there is a one-to-one correspondence from input stimuli to execution traces.
Thus, analyzing the different possible execution behaviors is tantamount to an analysis framework for multiple traces.

\subsubsection{Trace Compaction}
Traces describe the behavior of an application while computing a specific input stimulus (data). Often, the structure of the trace depends on the data itself, and might vary significantly, depending on the nature of the application.
If the calculation takes a long time, the trace will reflect this by being large.
Depending on the granularity and informational content of the events in a trace, even relatively small and simple computations can produce traces of a length that makes them impractical for storage and analysis.
However, the informational content, in the sense of the Kolmogorov Complexity, does not necessarily follow this behavior.
In fact, we believe that in most cases, there should be a very limited set of possible traces for a particular application. The set of such traces depends only of the application.
Similarly, in any deterministic model of computation, a trace itself only depends on the application description and the input data set. This very clearly gives bounds on the Kolgomorov Complexity of traces, which favors the hypothesis.
Thus, we believe if we can identify the relationship between the applications and the structure of its traces, we could find ways of describing traces in a compact and efficient fashion. 
This would have direct effects in the usefulness of all methods which are based on trace-analysis, which includes most state-of-the-art heuristics in the field of software synthesis.


 \begin{figure*}
 	\centering
 	\includegraphics[width=.76\textwidth]{figures/gantt.png} 
 	%	\vspace{-5mm}
 	\caption{A Gantt Chart of the Proposed Work Plan.}
 	\label{fig:gantt}
 		\vspace{-3mm}
 \end{figure*}

\subsubsection{Synthetic Traces}
In order to evaluate the efficacy of methods, as well as to apply learning-based methods, the behavior of many applications in different scenarios is required. 
Obtaining real benchmarks with large amounts of real data is a complex endeavor, which requires several person-years and is far beyond the scope of this thesis. 
An alternative solution is to leverage the informational structure of traces, similar to the methods for trace compaction.
By characterizing the common structure of traces and their relation to the application, we could leverage this structure to generate synthetic traces that describe realistic behaviors.
These synthetic traces would be useful in many scenarios, to test diverse methods or to produce enough data for others.

\subsection{Mapping Algorithms}
A final part of the software synthesis process which we will consider in this work are mapping algorithms.
Two components are involved in finding a mapping to execute an application.
The first issue is to identify objectives, like performance, resource utilization and energy consumption, and be able to determine, or at least estimate, the goodness of a mapping with regards to these objectives.
The second component of a good mapping algorithm is a method for navigating the exponentially-growing space of possible mappings, in order to find which perform best according to the desired objectives.
While the first described represents tremendous challenges and good engineering, here we will concentrate on this second part, since it is where the most mathematical structure of the problem lies. 

\subsubsection{Genetic Algorithms}
A class of algorithms that is well-suited for design-space exploration, such as is the case of mapping, are the so-called genetic algorithms.
In particular, inspired by the biological process of evolution, a subclass of algorithms called evolutionary algorithms (EA) shines by its usefulness in exploring very complex design spaces and finding Pareto fronts for multi-objective optimization.
Evolutionary algorithms have been applied to different instances of software synthesis successfully~\cite{Quan14}. However, existing EA do not fully exploit the mathematical structure of the mapping space.
As part of this work we plan to design EA that better utilize the mathematical structure of the mapping space that we have identified.

\subsubsection{Discrete Design Centering} 
It is very common in software synthesis that the goal of the synthesis is to maximize or minimize one or several concrete objectives.
Sometimes, however, the goal is for a system to continue to work reliably even in unexpected circumstances. Such a system is then said to be robust. 
We have successfully applied the concept of design centering to the mapping problem to find reliable mappings~\cite{hempel2017robust}.
However, in this exploratory work we also neglected much of the structure of the design-space of mappings, such as its discreetness, or a metric structure, i.e. as a metric space, that could be easily derived from an architecture description.
We believe by taking this additional structure into account, we can significantly improve the search methods for robust mappings.

\subsubsection{Compact Mappings}
Similar to the case for robust mappings, on multi-application scenarios, raw performance or energy-consumption are not always the desired goal. If several applications are to share a single system, special care has to be taken to balance the resource utilization of a mapping.
This principle is not only limited to the total amount of system resources utilized, but also usually their position, especially in modern systems with Network-on-Chip technologies.
In order to deal with this, the graph structures of applications and architecture representations can be leveraged to define mappings that are compact, for example.
We plan to utilize the structure described in this exposé to restrict search algorithms to find mappings with good spatial qualities in their resource utilization. This will be useful for increasing security and reliability by restricting or avoid contention on communication subsystems,
as well as an efficient resource utilization to leverage a single architecture for executing different applications dynamically.

\section{Workplan}
\label{sec:visit}

While the general methodology for this thesis has already been identified in previous work, several of the additional possible structures should be identified and we should strive to leverage them. This work is planned mostly in co-operation with colleagues in the group. Thereby, my principal task is to identify and describe the mathematical structure and propose methods to leverage it.
The concrete methods should be developed in unison with colleagues who better know the details and other the engineering aspects of a particular part of the problem. However, this allows concurrent work on several sub-projects from those described in the previous section.

Figure~\ref{fig:gantt} shows a Gantt chart of the proposed work plan. It starts with the projects I am currently working on, namely on the transformations and semantics. Both projects show promising results and we finish evaluating them, and then write up and publish them.
Continuing with the work on traces after finishing with graph semantics, I would start with synthetic traces, in order to have data to test the methods of the other projects. Then continue with multiple traces and trace compaction, which will probably benefit from the insights of understanding multiple traces.
In the field of mapping algorithms and representations, some of the present ideas have to be refined and implemented, but this can begin starting next year. Since we have previous work with design centering~\cite{hempel2017robust}, we would continue by implementing the methods there first and proceed to use them to generate compact mappings and improve genetic algorithms.
Work on symmetries, both new work concerning application symmetries, as well as continued work on the framework presented in this exposé will be subject of the time after the other projects mentioned. Similarly work formalizing models of architecture. Finally, six months have been allocated to wrap up all projects, and write a monolithic dissertation using all insight and approaches gained during the first year and a half.



%\section{Related Work}


%\section{Conclusions} 
%\bibliographystyle{ACM-Reference-Format}
\bibliographystyle{abbrv}
\bibliography{expose} 
\end{document}
