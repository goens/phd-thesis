Programming computers is notoriously difficult.
Indeed, people learning to program usually struggle, paradoxically, with the fact that the computer does precisely what they tell it to do. 
This is confusing not because a computer program is executed faithfully, but rather, because humans think at a very different level of abstraction.

It is certainly true that instructions in computer architectures are at a completely different level of abstraction than the instructions we give each other.
However, most programs are also not written at the level of the architecture.
Programming languages are designed with increasingly improving abstractions, to make it easier for programmers to express themselves.
Complementary to these efforts are compilers, which serve as bridge between the levels of abstraction.
Ideally, a compiler translates the abstract human-level expressions into efficient machine-level instructions.
While we have made significant progress, this task has proven to be dauntingly difficult.

Traditionally, we have put the resarch and effort into optimizing the execution of a single core.
Most of the progress of decades of research in programming language and compilers revolves around this single-core model.
In the last decade or two, however, with the multicore era, this challenge has increased dramatically.
Now we have to use and coordinate multiple cores, commonly with different capabilities.
The widespread programming language abstractions and compiler analyses of today are not well-suited to tackle this challenge.

\subsection{The Multicore Era}

On the hardware side, the last two decades have firmly established what we call the multicore era.
Modern computing systems are almost universally composed of multiple logical cores, and there is a clear trend of increasing both the number and the heterogeneity of these cores.
This increasing complexity brings about an increasing challenge in taming it.

\begin{figure}[h]
	\centering
   \resizebox{0.9\textwidth}{!}{\input{generated/moore.tex}}
   \caption{Chip trends as obtained from~\cite{chiptrends}. The lines present the exponential growth prediction if considering data up until the year 2000.}
	\label{fig:multicore_era}
\end{figure}


%Multicore Era
Both the execution frequency and the closely intertwined single-core processing speed of computing systems increased exponentially up until the early 2000s (cf. Figure~\ref{fig:multicore_era}), an empirical fact observed by Gordon Moore in 1965~\cite{moore}.
A programmer writing a piece of C code in the early 1970s would automatically benefit from the increasing processing speeds.
By recompiling her code for a processor $10 \times$ faster, she could very roughly expect her code also to run about $10 \times$ as fast.% Do I want to make this concrete (with Processor names?)

Since the early 2000s, however, while tranistor size continue to decrease, the exponential frequency scaling has stopped (cf. Figure~\ref{fig:multicore_era}).
Individual and clever designs have continued to improve single-processor speed, albeit at a significantly slower rate.
Instead, most improvements in microchips in raw processing power have come from a paradigm switch.
Hardware designers resorted to develop multicores, microchips composed of multiple logical cores that can execute in parallel.
Additionally, since different use-cases benefit from different computing core architectures, the inclusion of multiple chips has paved the way for heterogeneity.
This has resulted in a multicore era of computing.
Contemportary systems aimed at performance almost ubiqutiously consist of multicore chips, in many cases heterogeneous, which tremendously increases the system complexity.
The trend is clear, the number of cores, heterogeneity and overall system complexity will only continue to increase.

%something about network-on-chip, emerging memories, and hierarchical topologies
As the number of cores in a chip keeps increasing, the importance of coordination and communication between the cores raises as well.
Memory latency and bandwidth has been a bottleneck for many clasess of applications for a while.
In the case of manycores, which are multicores where the number of cores goes over several dozens, even to hunderds or thousands, the on-chip memory subsystem becomes a central design point of the chip.
Systems based on \ac{NoC} technology become necessary, lest a single bus interconnect becomes the bottleneck of the system when thousands of cores want to communicate simmultaneously through it.

\begin{figure}[h]
	\centering
   \resizebox{0.75\textwidth}{!}{\input{figures/haec.tex}}
   \caption{The HAEC architecture~\cite{HAEC} has multiple levels of hierarchy: on-chip, intra-board (optical links) and inter-board (wireless).} 
	\label{fig:multicore_era}
\end{figure}

In fact, for a multitude of reasons, manycores are commonly designed in a hierarchical fashion.
Smaller clusters of cores locally interconnected communicate between each other and off-chip via a larger \ac{NoC}.
These clusters and systems are usually heterogeneous as well, like the Karlay MPPA3 Coolidge~\cite{kalray_coolidge}, with includes accelerators for cryptography and secure cores, alongside general purpose cores.
Some systems even propose multiple layers of hierarchy, like the three layers of the HAEC topology~\cite{HAEC}, depicted in Figure~\cite{fig:haec}.
This is a proposed 3D stacked system with multiple boards each with multiple chips, not a single chip with this complex interconnect.
However, this design could allow for very low latencies, such that challenges of programming it are comparable to those of programming such a topology in an on-chip memory subsystem.
What happens if the mulicores in this system are similar to the Karlay MPPA3 Coolidge? This would yield more than $5000$ heterogeneous cores connected at five levels of hierarchy in different topologies.
More generally, complex topologies with possibly multi-level hierachies, add an additional layer of complexity to modern manycore systems besides heterogeneity and concurrency.

\subsection{Programming Multicores}

As already mentioned, programming is notoriously difficult since it translates from the level of abstraction of human interactions to the instructions of a computing system.
The multicore era greatly aggravates this already difficult problem.
Abstractions that proved very useful for programming single-cores become inadaquate.
An universal concept in programming is that of repeating an action multiple times, or looping. 
This perfectly exemplifies the differences in abstractions and how they become inadequate for multicore systems.

For example, consider the task of iterating through a bunch of pictures and determining which of them contain cats.
For instructing a human, we can probably say something like ``look through those pictures and sort out the ones have cats''.
A modern x86 chip, on the other hand, would understand something closer to this:

\begin{minted}{Nasm}
LBB0_1:                                 
	cmp	dword ptr [rbp - 56], 10
	jge	LBB0_4
	mov	edi, dword ptr [rbp - 56]
	call	_read_file
	mov	qword ptr [rbp - 64], rax
	mov	rdi, qword ptr [rbp - 64]
	call	_contains_cats
	movsxd	rcx, dword ptr [rbp - 56]
	mov	dword ptr [rbp + 4*rcx - 48], eax
	mov	eax, dword ptr [rbp - 56]
	add	eax, 1
	mov	dword ptr [rbp - 56], eax
	jmp	LBB0_1
LBB0_4:
	mov	eax, dword ptr [rbp - 52]
	mov	rcx, qword ptr [rip + ___stack_chk_guard@GOTPCREL]
	mov	rcx, qword ptr [rcx]
	mov	rdx, qword ptr [rbp - 8]
	cmp	rcx, rdx
	mov	dword ptr [rbp - 68], eax 
	jne	LBB0_6
\end{minted}

This snippet is a very oversimplified version of the task, but it serves to make the point.
Where we abstractly tell a human to look through the pictures and they undestand them as a whole set, interpreting themselves how to go through the set.
On the other hand, we instruct the machine to iterate through them by a series of very fine grained commands.
We need to set certain registers to contain the right memory addresses, before calling an instruction to operate on them.
Here we then call external functions that do the reading and cat identification.
To then loop through the pictures here, simplified, we repeat this reading and indetifying by jumping to a previous point in the sequence of instructions.
Even this x86 assembly snippet is already an abstraction, not only because it uses human-readable mnemonics for the instructions, but more so because it also abstracts away the concrete memory addresses.
In practice, however, almost no one would write this assembly code. Instead, they could write something closer to this (equivalent) C snippet:
\begin{minted}{C}
for(i = 0; i < N; i++){
    char *f;
    f = read_file(i);
    results[i] = contains_cats(f); 
}
\end{minted}

Notice how the register management and several other low-level details are abstracted away.
The end of the loop is very clear to read, as we know when we have reached the final picture.
We can certainly say this is at a level of abstraction between the human and machine instructions contrasted above.
However, the very widespread \texttt{for} instruction we used here also has the inherently sequential semantics exhibited by the machine code above.
The semantics of the for loop are that the loop body will execute completely.
After each iteration of the body, the increment expression is executed (usually incrementing the iteration variable), and the condition is evaluated, deciding wether to continue iterating.
Indeed, in the two (equivalent) snippets above, we don't know how the functions\texttt{read\_file} and \texttt{contains\_cats} work.
Do they have an inner state, or side effects?
We don't know if we can call \texttt{read\_file} in a different order, or multiple times in parallel.
Perhaps it is internally keeping a single reference to the iterator of the image files and doing so would break the logic.
The \texttt{for} instruction is very useful to abstract away the logic of registers and instruction jumps, but not a useful abstraction for expressing concurrency.
A similar construct exists in functional programming, \texttt{map}\index{\texttt{map} function}, which generally does not have this implicit sequential semantics.
The \texttt{map} instruction is what is called a higher-order function\index{higher-order function}, taking a function as an argument and applying it to a list or any iterable object, in general.
The same cat-identifying snippet, in Haskell, can be written as follows:
\begin{minted}{Haskell}
result = map (contains_cats . read_file) pictures
\end{minted}

On the other hand, there are reasons why Haskell is not the most widespread language for embedded systems.
One of them is that compiling such Haskell code to an efficient single-core execution is significantly more challenging than with the C code.
In general, we are faced with a trade-off between abstract expressability and translatability to an efficient execution.
This is where the challenge of engineering a 
%TODO: write properly
Difference between scince and engineering according to E.A. Lee~\cite{lee2017plato}.
Wadler argues about $\lambda$-calculus being discovered~\cite{wadler2015propositions}


\subsection{Software Synthesis}

% Software Productivity gap: Software Synthesis
Nowadays thus, a programmer writing a piece of (sequential) C code in the early 2000s cannot expect anywhere near a $100$-fold increase in performance today when she uses a microchip with $100 \times$ the number of transistors.
At least not without fundamentally restructuring her code to exploit parallelism.
More generally, the burden of maximally exploiting the hardware capabilities falls on the compiler and programmer, instead of coming automatically with the improvements in hardware.
This includes difficult problems like expressing parallelism and dividing tasks between different physical cores. %todo: perhaps discuss expressing parallelism

Inspired by hardware design flows, a family of methods called software synthesis aims to bridge the ensuing (software) productivity gap by integrating knowledge of the application and target multicore architecture into the compilation process.


Software synthesis refers to a family of methods deviced precisely to help with this burden of fully exploiting the capabilities of modern multicores.
At the core of these methods lies a shift in the programming model.
Instead of the de-facto sequential, shared-memory model, programmers express the code in diverse \acp{MoC}.
These models expose the structure of the computation in ways that permit a compiler to reason about its parallel execution, even in the prescence of heterogeneous hardware.
Aided by abstract models of the target archicecture, we can design compilers for multicore systems that devise execution strategies specialized to the target architecture and applications.
This can be realized for example by finding efficient mappings, i.e. allocations of computational and communication resources to the different parts of an application.

\subsection{Contribution}

This thesis seeks to improve the tools we use for understanding and tackling this problem.
It looks at it from both sides:
The compiler which aims to optimize the efficiency of code executing on a multicore system,
and the programming language abstractions and semantics striving for expressive idioms for programmers that permit the compiler to analyze them and execute them efficently.
While some of the difficulties are general of all computing systems, the largest challenges are usually specific to an application domain.
This thesis focuses on one such domain, of \acp{CPS} and other high-performance embedded systems running on \acp{MPSoC}.
In particular, we focus on a family of approaches colectively referred to as \emph{sofware synthesis}.


% Exploiting Structure in Dataflow Software Synthesis: Part I
The software synthesis process is centered around multiple formal constructs modeling the application and architectures, as well as decisions of the execution of the former on the latter, like mappings.
These are all rich in (mathematical) structure.
Architectures for example, even heterogeneous ones, exhibit a great deal of symmetry.
Similarly, the space of mappings has locality properties, where some mappings are similar whereas others are to others.
In this thesis we aim to identify and exploit these structures in different ways.
The first part of this thesis does this with a concrete software synthesis flow, which models applications as \acp{KPN} and finds mappings of these to heterogeneous systems.

% Other improvements changing the methods (Part II)
While Software Synthesis refers to a family of methods, each concrete method has a corresponding choice of abstractions it uses. Instead of exploiting the structure of the given abstractions, in the second part of this thesis we explore ways in which we can improve upon software synthesis methods by changing the structure we use: be it the semantics of the model of computation, implicit language structures or even the benchmarks we evaluate to asses our methods.

\todo{Add a discussion of the motivation to constraint in the right way. Do I have a good example? (GOTO?), cite~\cite{tasharofi2013scala}} 

\subsection*{Original Contributions}
This thesis presents the fruits of over half a decade of research on the subjects presented.
Research, especially in an interdisciplinary approach like presented here, is much more fruitful when collaborative.
In the case of joint work, I have made an effort to focus only on my own contributions in this thesis, whenever possible.
I have also taken care to describe the work of my colleagues as theirs, when I have included it as an indispensable requirement to understand my own work.
However,  many if not most of these ideas in these thesis are the result of joint work and cannot be credited to a single person.
In those cases I have also taken care to describe the work as joint and mention other co-authors.
If in doubt, any idea or result that I have included here which has already been published elsewhere is also due to my coauthors.

%\begin{figure}[h]
%	\centering
%   \resizebox{0.55\textwidth}{!}{\input{figures/placeholder.tex}}
%	\caption{A placeholder picture.}
%	\label{fig:placeholder}
%\end{figure}
%
%\end{figure}
%\begin{figure}[h]
%	\centering
%   \resizebox{0.55\textwidth}{!}{\input{generated/placeholder_plot.tex}}
%	\caption{Placeholder plot.} %something like: https://www.karlrupp.net/wp-content/uploads/2015/06/40-years-processor-trend.png
%	\label{fig:placeholder_plot}
%\end{figure}

%A placeholder reference\cite{goens_multiprog18}.
%\begin{figure}[h]
%	\centering
%   \resizebox{0.55\textwidth}{!}{\input{figures/placeholder_flow.tex}}
%	\caption{A placeholder flow.}
%	\label{fig:placeholder_flow}
%\end{figure}

