As we saw in Chapter~\ref{chap:mapping}, a central step of model-based software synthesis is a \ac{DSE} step for finding mappings, among others.
We know the mapping space is intractably large and complex and we cannot find the actual optima in the space for any real-life problem sizes.
The best we can hope for are near-optimal mappings in a reasonable amount of time.
Thus, we focus both on the quality of the mappings as well as the time required.
This section will focus on \ac{DSE} for finding near-optimal mappings, as defined in Section~\ref{sec:mapping_problem}. 
We will see many applications of the structures defined and analyzed in Chapter~\ref{chap:mapping_structures}

\subsection{Heuristics and Metaheuristics}
\label{sec:heuristics_vs_metaheuristics}

Generally in \ac{DSE} we distinguish between two approaches for dealing with these kinds of intractable problems, heuristics and meta-heuristics (cf. Section~\ref{sec:mapping_problem}).\index{meta-heuristics}
Recall that mapping heuristics are domain-specific algorithms that exploit the specific domain-knowledge to find a solution based on a pre-defined model of the problem, whereas meta-heuristics rely on an iterative evaluation of the points.
As we outlined above, different heuristics and meta-heuristics come with trade-offs between the exploration time required to find a solution and the quality of said solution.
This is certainly the case for many discrete optimization problems in general, the mapping problem being no exception~\cite{goens_mcsoc16}.
Commonly, meta-heuristics tend to find better results provided enough time, but require accordingly more time to do so.

A particular difficulty of comparing mapping approaches and algorithms are the different models used by different algorithms~\cite{goens_mcsoc16}.
With \mocasin we designed a common framework that allows us to compare between mapping algorithms~\cite{menard_rapido21}.
In particular, in \mocasin we have two heuristics for mapping: the \ac{GBM} heuristic~\cite{castrillon_dac12} and a static mapping variant~\cite{menard_rapido21} of the \ac{CFS} scheduler from Linux.
Additionally, we have implemented genetic algorithms based on and inspired by those found in Sesame~\cite{erbas2006multiobjective,quan2014towards,goens_mcsoc16}, a simulated annealing~\cite{simulated_annealing} mapping algorithm and a tabu search~\cite{tabu_search}.
\index{genetic algorithm}\index{simulated annealing}\index{tabu search}\index{random walk}
We also have a simple random walk algorithm for reference.
A survey of these mapping algorithms, among others can be found in~\cite{singh2013mapping}.
We implemented these algorithms for \mocasin and this thesis to have a basis for comparison from established literature.

We first compare these mapping algorithms to establish a baseline. 
We execute a random walk $500$ random iterations.
For the genetic algorithm we run an evolutionary $\mu + \lambda$ strategy for $20$ generations of population size $10$, crossover rate of $1$ with probability $0.35$ and mutation probability $0.5$, with a tournament selection with tournament size $4$.
For the \ac{GBM} algorithm we set the parameters as \texttt{bx\_m} of $1$, \texttt{bx\_p} of $0.95$, \texttt{by\_m} of $0.5$,\texttt{by\_p} of $0.75$, 
The simulated annealing heuristic we execute with an initial temperature of $1$ and a final temperature of $0.1$, with a temperature proportionality constant of $0.5$  and a random movement starting radius of $5$.
Finally, for the tabu search mapper we set a maximum of $30$ iterations, each of size $5$ and with a move set size of $10$ and tabu tenure of $5$, and a random candidate move update radius of $2$.
These parameters were chosen such that the mappers seemed to yield good result, yet not systematically (e.g. using something like Bayesian optimization or general (hyper-)parameter optimization approaches).
A deliberate choice in the parameters however is that the exploration times should be comparable between the meta-heuristics, i.e. such that the iterative mappers evaluate a similar amount of mappings.

\begin{figure}[h]
	\centering
   \resizebox{0.95\textwidth}{!}{\input{generated/heuristics_vs_metaheuristics.tex}}
	\caption{Comparison of multiple mapping heuristics and metaheuristics on the \ac{E3S} benchmarks.}
	\label{fig:heuristics_vs_metaheuristics}
\end{figure}

Figure~\ref{fig:heuristics_vs_metaheuristics} shows a comparison of the different heuristics and metaheuristics on the \ac{E3S} benchmarks.
Each of the metaheuristics that require random data we execute $10$ times and show the variation as calculated by the unbiased estimator of the standard deviation of the multiple sampled times.
The execution times vary obviously depending on the different benchmark applications and on the platforms they run on.
The absolute values of these times, however, are not interesting for comparing the mapping algorithms.
We thus norm the values of the execution times, taking the results of the genetic algorithms as baseline.
We then aggregate all values with the geometric mean.
The error bars in the plot are calculated by taking the average value $\pm$ the estimated standard deviation and norming each of the two extremes, the results of which are the extremes shown in the plot.

We first examine the results for the Odroid XU4 architecture.
The two heuristics find considerably lower results in average, but they do so in considerably less time.
More concretely, they yield about an order of magnitude worse results in about an order of magnitude less time.
The results of the random walk heuristic are significantly worse than those of the more structured metaheuristics, even though it takes a comparable amount of time.
This is due to a deliberate choice, since as explained above, the number of random mappings was chosen specifically to be comparable to the number the number of mappings evaluated by the other meta-heuristics.
Since $500$ mappings is not a small amount, it is not terribly surprising that the random mapper beats the two heuristics.
Finally, the best mappings are found by the simulated annealing meta-heuristic, albeit only by $3\%$ compared to the genetic algorithm.

When we turn our attention to the significantly larger and more complex MPPA3 Coolidge architecture, we see that the picture changes drastically.
The marked difference between heuristics and meta-heuristics disappears in this case.
The \ac{GBM} heuristic is on par with the random walk results in average, while taking substantially less time.
This is simply explained by the significantly larger design space of mapping to the MPPA3 Coolidge.
In this case, the genetic algorithm significantly outperforms both other meta-heuristics, by a factor of $4-5$, while taking less time.
The most striking result here, however, is the extremely good performance of the static CFS heuristic.
This good performance is misleading at first, a perhaps more honest assessment of the results is that \emph{all other (meta-) heuristics perform poorly}.
We can interpret this as a consequence of the growing design space and its complexity, which affects the metaheuristics, while the static CFS mapper can still leverage domain-specific knowledge to find fairly good mappings.

TODO: add and test genetic with CFS initials.

\subsection{Leveraging Symmetries}

As motivated when discussing them in Section~\ref{sec:symmetries}, symmetries can be used to improve \ac{DSE} in the mapping problem.
There are two distinct applications of symmetries in \ac{DSE}.
The first application is for speeding up meta-heuristics (without modifying them), as shown in~\cite{goens_taco17}, by leveraging the equivalence of symmetric mappings in a symmetry-aware cache.
The second application is by pruning the design space as seen by the meta-heuristic~\cite{goens_mcsoc18,goens_tcad21}, effectively changing the meta-heuristic.

We will first discuss the idea of a symmetry-aware cache.\index{symmetry-aware cache}
As discussed before, meta-heuristics work through an iterative principle, where they evaluate mappings and drive the search based on the results of the evaluation.
While the evaluation is fast and light-weight by design (cf. sections~\ref{sec:sec:simulating_mappings} or~\ref{sec:architecture_models}), it still usually dominates the execution time of the exploration (cf. Figure~\ref{fig:heuristics_vs_metaheuristics}).
A defining property of the symmetries is how simulation results are invariants of the equivalence classes of orbits (cf. Section~\ref{sec:symmetries}).
This means that if we know the results of a simulation for a mapping, we know the results for all mappings in its equivalence class.
We can leverage this by designing a symmetry-aware mapping cache, which stores simulations results by equivalence class instead of by mapping~\cite{goens_taco17}.
This yields a trade-off, where computations about the symmetry have to be executed every time a mapping is going to be looked-up in the cache or evaluated.

We implemented a symmetry-aware cache in \mocasin which uses \mpsym and its Python interface.
We used this to evaluate the method of symmetry-aware caching on the \ac{E3S} benchmarks by accelerating the various meta-heuristics discussed in Section~\ref{sec:heuristics_vs_metaheuristics}.
We can also evaluate the domain-specific methods of mpsym~\cite{goens_tcad21} by applying this method to multiple architecture topologies.
In addition to the Odroid XU4 and MPPA3 Coolidge, which we used consistently throughout this thesis, we also test the methods on the exploration of two additional architectures: \ac{HAEC} and a simple generic cluster.
The \ac{HAEC} architecture (cf. Figure~\ref{fig:HAEC}) is a \ac{PCB} design with low-latency optical interconnects on layers with a $4 \times 4$ regular-mesh structure.
Four such layers are then connected, using low-latency wireless interconnects to communicate between adjacent layers.
While in the \ac{HAEC} design each node of the layer is an \ac{MPSoC}, we model the topology by placing cores in those nodes and considering the board as a single \ac{MPSoC}.
This serves to evaluate our methods on this topology.
The generic cluster architecture we evaluate is the simplest non-trivial clustered architecture topology.
It consists of two identical clusters, each of which with two identical cores.
Each cluster shares a cache, and the two clusters can communicate over main memory.

\begin{figure}[h]
	\centering
   \resizebox{0.95\textwidth}{!}{\input{generated/symmetries_cache.tex}}
	\caption{The effect of a symmetry-aware cache on multiple architecture topologies as evaluated on the \ac{E3S} benchmarks.}
	\label{fig:symmetry_cache}
\end{figure}


Figure~\ref{fig:symmetry_cache} shows the results of evaluating a symmetry-aware cache on the different meta-heuristics on different architecture topologies.
It reports the execution time of the full exploration, separating the overhead from symmetry calculations from the rest of the exploration time for the symmetry-aware cache.
These relative times are normed such that the regular exploration has a time of $1$. Both variants were executed $10$ times with identical seeds, for each benchmark. 
Thus, they executed the exact same exploration, evaluating the exact same mappings and returning the same result.

Unsurpisingly, the symmetry-aware cache does not offset the overhead of symmetry calculations for the random walk meta-heuristic.
Since the mapping space is extremely large, the probability of finding two equivalent mappings at random is quite small. 
Thus, both a symmetry-aware cache and a regular cache are not very useful for an unstructured random walk.
The other meta-heuristics are much more structured, and clearly do benefit from the speedup.
For the Odroid XU4, the symmetry-aware cache consistently yields a large speedup of the exploration time, around $1.4-1.7 \times$.
For the simple cluster and the \ac{HAEC} architectures, the results are similar.
The best results are achieved for the MPPA3 Coolidge topology, which despite its complexity has a very well-defined structure we can exploit with our wreath-product construction (cf. Section~\ref{sec:symmetries}).
For the simulated annealing meta-heuristic, our symmetry-aware cache sped up the simulation in \textbf{average} by $14.5 \times$!

We see that our \mpsym-powered symmetry-aware cache is very useful for speeding up \ac{DSE}.
It can still be improved, however.
In~\cite{goens_taco17} we also included application symmetries in the cache, which actually made a significant difference, as we were not using the more optimized algorithms from \mpsym.
Application symmetries have a much potential, which is also intuitive if we consider the cardinality of the mapping space $|V_A|^{|V_K|}$ grows exponentially with the number of processes.
However, as explained in Section~\ref{sec:app_arch_symmetries}, we have no systematic method of reliably detecting these application symmetries yet, which is why we do not include them in \mocasin.
The other improvement comes from partial permutations. While we described them in Section~\ref{sec:partial_symmetries} and have a partial implementation in \mpsym, we still need efficient algorithms on partial permutations for exploiting the partial symmetries of the mapping space.
This is evident in the comparison between the results of the MPPA3 Coolidge and \ac{HAEC} topologies in Figure\ref{fig:symmetry_cache}.
The \ac{HAEC} topology has partial symmetries in each of the meshes (cf. Section~\ref{sec:partial_symmetries}), and it also has partial symmetries in the symmetry group of the clusters (the larger group from the wreath product).
Since the first and last layers cannot communicate with each other directly (the topology is not toric), the symmetries of the clusters are all only partial.

We can also leverage symmetries in \ac{DSE} by changing the underlying space as seen by the meta-heuristic.
We also implemented this in \mocasin by using the \texttt{SymmetryRepresentation} as described in Section~\ref{sec:representations}.
In this way, the meta-heuristics see the factor space and are effectively changed in their operations.
We tested this with the same setup as before, the results of which can be seen in Figure~\ref{fig:symmetries_changed}.
It shows both, the relative results of the mapper (in terms of the runtime of the best mapping) and the relative exploration times.

\begin{figure}[h]
	\centering
   \resizebox{0.95\textwidth}{!}{\input{generated/symmetries_changed_operations.tex}}
	\caption{The effect of a symmetry-aware cache on multiple architecture topologies as evaluated on the \ac{E3S} benchmarks.}
	\label{fig:symmetry_changed_operations}
\end{figure}

\begin{figure}[h]
	\centering
   \resizebox{0.95\textwidth}{!}{\input{generated/symmetries_coolidge_changed_operations.tex}}
	\caption{The effect of a symmetry-aware cache on multiple architecture topologies as evaluated on the \ac{E3S} benchmarks.}
	\label{fig:symmetry_coolidge_changed_operations}
\end{figure}

\subsection{Leveraging Metric Spaces}
Improving DSE algorithms with metric spaces?

Talk about gradient descent: why it does not work w/o metric space structure, and why it should with it.
\begin{figure}[h]
	\centering
   \resizebox{0.95\textwidth}{!}{\input{generated/geometric_heuristics_coolidge.tex}}
	\caption{The effect of embedding-based representations in metaheuristics that leverage the geometry on the MPPA3 Coolidge platform.}
	\label{fig:coolidge_geometric}
\end{figure}

We can take two observations from these results and combine them.
The first one is the knowledge that geometry-based heuristics indeed benefit from a better metric, independent of if the resulting heuristics are overall good.
The second observation is more subtle, it's about the general structure of the design space. 
The design spaces of mappings seem to consist of multiple islands of performance with similar properties, separated by 
An observation we can make is that 

\begin{figure*}[t]
  \centering
	\begin{subfigure}[b]{0.43\textwidth}
    \includegraphics[width=\textwidth]{figures/coolidge-af-space-sim-vec.png}
		\caption{SimpleVector}
		\label{fig:lvars-bench-cores}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.43\textwidth}
    \includegraphics[width=\textwidth]{figures/coolidge-af-space-emb-sym.png}
		\caption{SymmetryEmbedding}
		\label{fig:lvars-bench-overhead}
	\end{subfigure}

	\caption{Visualization of the same design space of the audio filter benchmark on the MPPA3 Coolidge platform in two different representations.}%
	\label{fig:visualization_simpvec_symemb}
\end{figure*}

\subsection{All Representations}
\begin{figure}[h]
	\centering
   \resizebox{0.95\textwidth}{!}{\input{generated/multiple_representations_exynos.tex}}
	\caption{Comparison of the effects of multiple representations on the Odroid XU4 platform.}
	\label{fig:multiple_representations_exynos}
\end{figure}

\begin{figure}[h]
	\centering
   \resizebox{0.95\textwidth}{!}{\input{generated/multiple_representations_coolidge.tex}}
	\caption{Comparison of the effects of multiple representations on the MPPA3 Coolidge platform.}
	\label{fig:multiple_representations_exynos}
\end{figure}