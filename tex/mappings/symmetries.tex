In this chapter we will explore the mathematical structure of symmetry in the software synthesis process, mostly the work published in ~\cite{goens_iess15,goens_taco17,goens_scopes17,goens_mcsoc18,goens_tcad21}.
The material in this section makes use of concepts in group theory.
We assume the basic concepts as seen in any undergraduate course on group
theory, with the definitions of groups, actions and orbits.
A brief introduction, to the level required by this chapter, can be found in Appendix~\ref{appendix:groups}. 

\subsection{Architectures and Applications}
\label{sec:arch_app_symmetries}

Intuitively, when we say an object is very symmetric we usually mean it has parts that are similar or identic, and the object looks identical (or similar) from multiple points of view.
In a symmetric face, for example, both the left and right sides of the face are similar. A hexagonal mosaic might look the same when seen from six different angles.
Mathematically, this is commonly modeled through transformations. A reflection along the vertical axis in a face, or rotations of $60^\circ$ in the heaxgon, both leave the object (mostly) unchanged.
We can do the same for hardware architectures, even heterogeneous ones.

For example, the Exynos~5 in the Odroid-XU4 has four identical Cortex A7\texttrademark, say $\PE_1,\ldots,\PE_4$ and four identical Cortex A15\texttrademark cores, say $\PE_5,\ldots,\PE_8$.
A transformation that swaps the cores $\PE_1$ and $\PE_2$ leaves the archtiecture topology unchanged, since the cores are identical. This is depicted in Figure~\ref{fig:exynos_symmetries}.
On the other hand, a transformation that swaps $\PE_1$ and $\PE_5$ does change the topology, since the cores are of different types. As can also be seen in Figure~\ref{fig:exynos_symmetries}.
\begin{figure}[h]
	\centering
\resizebox{0.3\textwidth}{!}{
   \begin{tikzpicture}
     \input{figures/odroid}
   \end{tikzpicture}
 }
   \caption{Symmetries in the Odroid-XU4 architecture. TODO: fix figure.}
   \label{fig:exynos_symmetries}
\end{figure}

When the memory subsystem is more complex, this is also reflected in the topology. Consider the \ac{NoC}-based architecture depicted in the example, with four identical cores $\PE_1\ldots,\PE_4$.
An analogous transformation to the one described before, which swaps the cores $\PE_1$ and $\PE_2$, is not a symmetry of this topology, as depicted in Figure~\ref{fig:non_equivalent}.
The change in the cores changes the communication patterns.
Before the transformation, sending data from $\PE_1$ to $\PE_3$ needs two hops over the \ac{NoC}, whereas after the transformation it can be sent within a single hop.

\begin{figure}[h]
	\centering
\resizebox{0.3\textwidth}{!}{
   \begin{tikzpicture}
     \input{figures/odroid}
   \end{tikzpicture}
 }
   \caption{The communication topology affects symmetries in architectures.}
   \label{fig:non_equivalent}
\end{figure}

Generally, the transformations that preserve the structure of the architecture topology have a clear structure.
If two transformations $t_1$ and $t_2$ preserve the structure of the architecture topology, then their composition $t_1 \circ t_2$ also preserves it.
Similarly, it is clear that reversing a transformation $t_1^{-1}$ also preserves the structure.
Finally, the identity transformation on the architecture $\operatorname{id}_A$ (which does not change anything) clearly preserves the structure.
These observations together mean that these transformations have the structure of a group with the function composition $(\circ)$ as its operation.

More precisely, the group of symmetries of the achitecture is precisely the group of graph isomorphisms from the architecture graph $A$ to itself.
An isomorphism from an object to itself is called an automorphism\index{automorphism}.
We denote the group of automorphisms of the architecture $A$ as $\Aut(A)$

For the case of the \ac{NoC}-based architecture, the authomorphism group $\Aut(A\text{NoC}) \cong D_4$ is a dihederal group on $4$ points.
It conists of $3$ rotations, $4$ reflections and the identity transformation.
The Odroid architecture, on the other hand, has $\Aut(A_\text{Odroid}) \cong S_4 \times S_4$ as symmetry group.
This group with $48$ transformations is comprised of (independent) arbitrary permutations of the A15 and A7 cores.

Since the Odroid architecture is heterogeneous, both clusters are distinct and there is no symmetry between them.
Many complex architectures, however, do consist of multiple identical culsters. Consider the architecture depicted in Figure~\ref{fig:coolidge}.
It is the MPPA3 Coolidge from Kalray~\cite{coolidge} and consists of $5$ identical clusters.
Each cluster has $17$ cores, $16$ of which are identical general-purpose cores, and the last one is a special-purpose secure and management core.

The MPPA3 Coolidge is a hierarchically-designed architecture. The five identical clusters are conceptually at a different level than the cores at each cluster.
Designs like the HAEC~\cite{HAEC} topology mentioned in the introduction (cf. Figure~\ref{fig:haec}) have even more levels of hierarchy.
The symmetries of these hierarchical architectures are reflected in the different levels of hierarchy of the topology~\cite{goens_tcad21}.
For example, the automorphism group of the MPPA3 Coolidge is $\Aut(A_\text{Coolidge}) \cong S_{16} \wr S_5$ and has $16! \cdot 5! \approx 2.51 \cdot 10^{15}$ symmetries. 

So far we have discussed the symmetries of architectures.
However, we can apply the same principle to applications and their graphs.
Conisder the audio filter example application from Section~\ref{sec:kpn_basic} (cf. Figure~\ref{fig:audio_filter_graph}). 
The left and right channels perform precisely the same computation on different data.
We could not, for example, just swap the \texttt{fft\_l} and \texttt{fft\_r} nodes, since that would result in a different application that also swaps the channels of the audio file.
On the other hand, if we swap the whole subgraph consisting of \texttt{fft\_l}, \texttt{filter\_l} and \texttt{ifft\_l} with the equivalent subgraph of \texttt{fft\_r}, \texttt{filter\_r} and \texttt{ifft\_r}, the application remains identical.
This is depicted on Figure~\ref{fig:audio_filter_symmetries}.

\begin{figure}[h]
	\centering
\resizebox{0.9\textwidth}{!}{
\begin{tikzpicture}
   \input{figures/audio_filter_graph}
 \end{tikzpicture}
}
   \caption{A symmetry of the audio filter application. TODO: adapt figure}
	\label{fig:audio_filter_symmetries}
\end{figure}

Mathematically, we need to model the semantics of the application to reflect its symmetries.
For an application $K = (V_K,E_K)$ we can label the nodes $V_K$ with unique identifiers relating them to the $KPN$ process that execute them (e.g. the \texttt{\_\_PNprocess} in a \ac{CPN} program). 
Formally, thus, the automorphism group $\Aut(E)$ of the labeled graph $K$ is trivial, i.e. $\Aut(E) = \{ \id \}$. 
We could label $K$ differently to capture the symmetry from Figure~\ref{fig:audio_filter_symmetries}.
For example, if use the source code of the process as label, we would capture this symmetry.
We have to be more careful, however, as this can lead to a problematic definition of symmetries.

An application might use the same code at different points, resulting in very different behavior.
For example, consider an application that receives a list of points, which it sorts before operating on it.
Before returning the list, it sorts them again to ensure they are sorted.
Both times it sorts the list using the quicksort algorithm, yet the second time the list is almost always sorted or close to being sorted.
Then the execution of the same quicksort code in the second instance behaves very differently from the first time.

A difference like the one outlined above is very difficult to capture automatically, as it requires understanding of the application to a very high level of abstraction.
We thus consider application symmerties as manually-defined anotations.
There are some conceivable ways to automatically capture and annotate such application symmetries, for example when dealing with known data-level parallelism.
In future work, a framework as we discuss in Chapter~\ref{chap:mocs}, Section~\ref{sec:macqueen} could be extended to extract symmetries.
For the rest of this thesis, however, we focus on symmetries induced from the architecture.

\subsection{Mappings}

We have seen how the architecture and applications have symmetries in their structure. 
The groups $\Aut(A)$ and $\Aut(K)$ act on the architecture $A$ and the application $K$, respectively.
These actions also induce an action on the mapping space. 
Let $m : K \rightarrow A$ be a mapping.
Recall that a symmetry $\sigma \in \Aut(A)$ of the architecture is a transformation that leaves the structure of the architecture unchanged.
Conisder then the mapping $\sigma m := k \mapsto \sigma(m(k))$.
Since the structure of $A$ is unchanged, then the structure of $m$ and $\sigma m$ is also identical.
All observable properties of $m$ and $\sigma m$, like the execution time or energy consumption, are the same.
If they were not, it would have to be due to a structural difference in the (sub)architectures $m(K),(\sigma m)(k) = \sigma(m(K)) \leq A$, which are isomorphic by assumption on $\sigma$.
We say that these properties like the execution time and energy consumption are \emph{invariants}\index{invariants} of the group action.

Figure~\ref{fig:mapping_action_example} shows an example of the action on mappings. \todo{add and describe figure.}

The case for $K$ is analogous.
Let $\pi \in \Aut(K)$ be a symmetry of the application.
Then the mapping $\pi m := k \mapsto m(\pi^{-1}k)$ is equivalent to $m$.
Note that we define it with $\pi^{-1}$ instead of $\pi$ so that this defines a left action.
Indeed, for $\pi, \tau \in \Aut(K)$, we have
\begin{align*}
  (\pi (\tau m))(k) = \pi m(\tau^{-1}k) = m(\pi^{-1}\tau^{-1}k) = m((\tau \pi)^{-1}k) = ((\tau \pi) m))(k) 
\end{align*}

From the models of the problem, as defined in Chapter~\ref{chap:mapping}, mappings under these symmetries are necessarily indistinguishable from each other, since they rely on inherent symmetries of the models.
On the other hand, these are just models, they need not reflect reality.
It still leaves the question open, how does this hold up in pracitice?
Are equivalent mappings actually equivalent?
In~\cite{goens_scopes17} we tested this empirically, by executing four equivalent mappings and measuring the runtime and energy.
We tested four equivalent mappings of the audio filter benchmark on an Odroid XU3 and executed each $50$ times, measuring three metrics.
The Odroid XU3 is almost identical to the Odroid XU4, but features on-board energy sensors.
We measured the \acs{CPU} time, which is the total aggregate time spent by all \acsp{CPU} executing the application.
We also measured the wall-clock time, which is the total time of the application, measured from start to finish as it would pass on a wall clock.
Finaly, we measured the total energy over the INA-231 sensors connected over the I2C bus, as the aggregate of the energy measured at each individual component, sampled at $10$~Hz.

\begin{figure}[h]
	\centering
\resizebox{0.95\textwidth}{!}{
     \input{generated/mappings_equivalent}
 }
   \caption{Measurements of four equivalent mapings for the audio filter application on the Odroid-XU3 architecture.}
   \label{fig:symmetry_measurements}
\end{figure}

Figure~\ref{fig:symmetry_measurements} shows the results of the measurements as box-and-whiskers plots.
We see that the execution times of all mappings are well within the ranges of each other, both when measured as wall-clock or \ac{CPU} time.
We can test this in a more rigurous fashion with an \ac{ANOVA} F-test.
For the wall-clock data, we get a probability of $p = 5.06\%$, rejecting the null hypothesis that variance in the wall-clock times from different mappings is explained by more than statistical variance.
For the \ac{CPU} time data, the value of $p=15.2\%$ it is even clearer that the variance is likely just statistical.
We can conclude that, at least for this example, equivalent mappings indeed seem to have the same execution time behavior.

The energies of the mappings are less clearly distinct, especially for the first two mappings, yet they all still require comparable amounts of erengy.
An \ac{ANOVA} F-test yields a $p$ value of $0.009\%$.
It is worth noting here that these measurements are just a first estimation.
The accuracy of the energy data depends on a multitude of factors like the sampling rate or the measurement accuracy of the INA-231 sensors.
We have not made a proper assesment of measurement errors and error propagation, which is especially large in the energy measurements that span measurements from four different components.
Precise energy measurments, however, are beyond the scope of this thesis.
We this experiment we cannot really conclude nor discard the hypothesis that energy consumption is an invariant of the mapping symmetries.

\subsection{Calculating Symmetries}

The branch of mathematics known as computational group theory deals with computational aspects of the theory of groups, which we have used here for formalizing the symmetries of applications, architectures and mappings. 
An overview of the methods of computational group theory can be found in~\cite{holt,seress2003permutation}, which both cover far beyond the basics presented in this subsection.
Here we will present only the methods necessary for the calculations required of applications to software synthesis.

To leverage the symmetries explained in this theses for software synthesis, we need to methods to calculate the following:
\begin{enumerate}
\item\label{prob:autgrp} Given an architecture graph $A$, the group of symmetries $\Aut(A)$.
\item\label{prob:orbit} Given a mapping $m : T \rightarrow A$ and the symmetry group $G := \Aut(T \rightarrow A)$, enumerate the orbit $Gm$.
\item\label{prob:equiv} Given two mappings $m,m' : T \rightarrow A$ and the symmetry group $G := \Aut(T \rightarrow A)$, determine whether $m = gm'$ for a $g \in G$, i.e. if the two mappings are in the same orbit.
\end{enumerate}

Mature software exists for computational group theory that can, in principle, solve these problems. 
The \acs{GAP} system is a \ac{DSL} for computational discrete algebra with a focus on (computational) group theory~\cite{gap}.
We developed algorithms for dealing with problems \ref{prob:autgrp}-\ref{prob:equiv} in \ac{GAP}~\cite{goens_taco17}.
We also included naive versions of most algorithms implemented directly in Python in \mocasin.

Using \ac{GAP}-based algorithms in software synthesis tools like \ac{MAPS} in practice, however, comes with a series of complications.
The largest problem is that it adds a dependency on a whole ecosystem.
A complete distribution of \ac{GAP} is over $200$ MiB of size and takes around a second to start up in standard commodity hardware of today.
Additionally, to communicate with a running \ac{GAP} instance we need to use \ac{POSIX} pipes, which is cumbersome and not portable.
A project existed to use \ac{GAP} as a library, but seems to be unmaintained, and we could not get it to function.
We thus developed a standalone library~\cite{nicolai_studienarbeit}, \mpsym , which implements the required algorithms to solve problems ~\ref{prob:autgrp}-\ref{prob:equiv} and includes a domain-specific extension for efficiently dealing with hierarchical (e.g. clustered) archictures~\cite{goens_tcad21}.

Calculating the group of symmetries from an architecture graph (Problem~\ref{prob:autgrp}) is very related to the graph isomorphism problem. 
This is a problem in NP, and it is not known, neither believed to be in P nor NP-complete. 
In December 2015, L\'{a}sl\'{o} Babai published a pre-print where he claims to have found a quasi-polynomial algorithm~\cite{babai_graph_isomorphism}, 
yet at the time of this writing (January 2021) the peer-review is still not complete.
Regardless of the worst-case complexity, graph isomorphism can be solved efficiently in practice for most instances~\cite{McKay201493}.
Algorithms for doing so are implemented in nauty/Traces, which \mocasin and \mpsym use to solve Problem~\ref{prob:autgrp}.

Most algorithms in computational group theory use a special data structure describing the group.
This data structure is called a \emph{\ac{BSGS}}, see~\cite{holt,seress2003permutation} for more details.\index{base and strong generating set}
The standard algorithm for calculating the \ac{BSGS} for a group is the Schreier-Sims Algorithm\index{Schreier-Sims Algorithm}.
Multiple variants of this algorithm exist, which are more efficient under different circumstances.
\acp{CAS} like \ac{GAP} use different variants with sophisticated heuristics for selecting which variant to use.
In \mpsym we implement some variants of the Schreier-Sims with less sophisticated selection, which do not surpass \ac{GAP}'s performance.
For all groups investigated in this thesis, however, \mpsym was comparable to \ac{GAP}, without the large ecosystem dependency~\cite{nicolai_studienarbeit,goens_tcad21}.

Problem~\ref{prob:orbit} is a standard problem in computational group theory.
We solve it using the Orbit algorithm, which can easily be adapted to a lazy variant, described in Algorithm~\ref{label:algo:lazy_orbit}.
If we use a perfect hash, the algorithm returns exactly the orbit of the mapping.
If the hash can have duplicates, a smaller orbit might be returned, but the algorithm will clearly never yield elements from outside the orbit.
This lazy variant is especially useful when looking for any mapping in the orbit which fullfils some properties, instead of being interested in the full orbit.
This is especially useful in the \acs*{TETRiS} system, which we will describe in Section~\ref{sec:tetris}.

\begin{algorithm}
	\caption{A lazy variant of the standard orbit algorithm}
	\label{algo:lazy_orbit}
	\begin{algorithmic}[1]
	  \Input{A generating set $X = (g_1, \ldots, g_n), \langle g_1, \ldots, g_n \rangle = \operatorname{AutGrp}(M)$ for the mapping space, a mapping $m_0$. }
	  \Output{The orbit of $m_0$: $\operatorname{AutGrp}(M)m = \{ g m_0 \mid g \in \operatorname{AutGrp}(M) \}$ }
     \State H $\leftarrow$ $\{ \operatorname{Hash}(m_0) \}$
     \State CurElems $\leftarrow$ $\{ g_i m_0,~\operatorname{Hash}(g_i m_0) \notin H \mid i = 1,\ldots,n \}$
     \State H $\leftarrow$ $H \cup \{ \operatorname{Hash}(m) \mid m \in \text{CurElems} \}$
     \While{ CurElems $\neq \emptyset$}
	     \For{ $m \in \text{CurElems}$}
		         \State \textbf{yield }$m$
	     \EndFor 
     \State CurElems $\leftarrow$ $\{ g_i m,~\operatorname{Hash}(g_i m) \notin H \mid m \in \text{CurElems}, i = 1,\ldots,n \}$
     \State H $\leftarrow$ $H \cup \{ \operatorname{Hash}(m) \mid m \in \text{CurElems} \}$
   \EndWhile
	\end{algorithmic}
\end{algorithm}

Finally, to solve Problem~\ref{prob:equiv}, we could simply solve Problem~\ref{prob:orbit} for both elements and see if the orbits are identical.
Orbits form a partition of the mapping space $M$, meaning that two orbits are either identical or dijsoint (and the union of all orbits yields $M$).
This is a very inefficient way of solving Problem~\ref{prob:equiv}, since it means we have to enumerate the whole orbit for each element.
Using the same principle of the Orbit's partition, we can also just enumerate the orbit for one element and see if the other element is in it.
While this is also an improvement, it is still very inefficient. Orbits can be very large when the problem has much symmetry.
By default, \mpsym uses this variant as a fall-back method to solve Problem~\ref{prob:equiv} when correctness needs to be guaranteed.

Another alternative for this which works without enumerating any orbits is based on the fact that the symmetries of the mapping $\operatorname{AutGrp}(M) \leq \operatorname{Sym}(M)$, the symmetric group on $M$ (i.e. the group of all permutations on $M$).
Thus, if two mappings $m, m'$ are in the same orbit under $\operatorname{AutGrp}(M)$, then they are also in the same orbit under $\operatorname{Sym}(M)$: there exists a permutation $\sigma \in S_{|M|}$ which takes $m$ to $m'$.
However, $|M|$ as we have seen can be very large, as it grows (at least) as $|V_A|^|V_K|$ and $\operatorname{Sym}(M) \cong S_{|M|}$ is thus unimaginably large, namely $|\operatorname{Sym}(M)| = |M|! \geq (|V_A|^|V_K|)!$.
If we consider only architecture symmetries, this all works over the much smaller $\operatorname{AutGrp}(A)$.  We obviously do not have to iterate over the group to construct $\sigma$, since we know both $m,m'$ we can construct it directly.
Knowing $\sigma$, we can efficiently solve the \emph{group membership problem}~\cite{seress2003permutation} for these permutation groups, using the \ac{BSGS}. \index{group membership problem}
We know, namely, that $\sigma$ is in $\operatorname{AutGrp}(M)$ if and only if $\operatorname{AutGrp}(M)m = \operatorname{AutGrp}(M)m'$, by the definitions of the orbit and $\sigma$.
On the other hand, if we cannot construct $\sigma$ from $m,m'$, because it leads to contradictions, then, obviously, the orbits are different.

We did not implement this variant based on the group membership problem.
The reason for this is that we implemented an alternative which also allows us to select a mapping to work with, e.g. for \ac{DSE}.
It is based on canonical representatives~\cite{goens_taco17,goens_mcsoc18}.
A canonical representative of an orbit $Gm$ is an element $m_0 \in Gm$ such that there is a function
$f : G \setminus \setminus M \rightarrow M$ which maps $Gm$ to $m_0$.
In other words, the function $f$ selects a unique element of every orbit, this element is the canonical representative.

For constructing our canonical representatives, we order mappings using the \emph{lexicographical ordering}.
For two mappings $m = (m_1,m_2,\ldots,m_k), m' = (m'_1,\ldots,m'_k)$ we say that $m \leq m'$ if and only if there exists a $j \leq k$ such that $m_i = m'_i$ for all $i < j$ and $m_j \leq m'_j$.\index{lexicographical ordering}
The function $f$ for the canonical element of the orbit thus maps $Gm$ to $\min Gm$.
In other words, we choose canonical elements to be the lexicographical-minimal elements of the orbits.

\begin{algorithm}
   \caption{Local search for finding canonical representatives. Adapted from Algorithm~1 of~\cite{goens_mcsoc18}.}
   \label{algo:canonical_representatives}
   \begin{algorithmic}[1]
     \Input{A mapping $m$, a generating set $S$, with $\langle S \rangle = \operatorname{AutGrp}(M)$.}
     \Output{A mapping $m_\text{canonical} = gm$ with $m_\text{canonical} < m'$ for all $m' \in Gm$}
     \State $F \gets \{m\}$
     \State $F_\text{old} \gets \emptyset$
     \While {$F \neq F_\text{old}$}
     \State $F_\text{old} \gets F$
     \ForAll{ $s \in S$ }
     \ForAll{ $m' \in F$ }
     \If {$sm < m$}
     \State $F \gets sm$
     \EndIf
     \EndFor
     \EndFor
     \State \label{line:optional} \textbf{(optional)} $F \gets \{ \operatorname{min}_{ m' \in F} m' \}$
     \EndWhile
     \Return $\operatorname{min}_{ m' \in F} m'$
   \end{algorithmic}
 \end{algorithm}
 
To find the lex-minimal canonical representatives we use a local-search algorithm based on an iteration similar to the Orbit Algorithm. 
Algorithm~\ref{algo:canonical_representatives} shows this local-search heuristic. 
This algorithm to returns the lex-minimal element of the orbit if the generating set has a particular property, which we called being a \emph{strictly order-preserving generating set}~\cite{goens_mcsoc18}.
We say $S$ is a strictly order-preserving generating set if for two mappings $m' < m$ in the same orbit, i.e. $m \in \langle S \rangle m'$, there exists a word $s_1,\ldots,s_n$ in the generators $s_i \in S$, such that $m' = s_1 \ldots s_n m$ with $s_i (s_{i+1}\ldots s_n)m < (s_{i+1} \ldots s_n)m$ for all $i = 1,ldots,n-1$.\index{stricly order-preserving generating set}
For example, for the symmetric group $S_n$, the set of all transpositions $S = \{ (i,j) \mid i \neq j \in \{1,\ldots,n\} \}$ is such a strictly order-preserving generating set.
Without this property, the local search could yield an element which is not the lex-minimal element.
If we remove the optional reduction in Line~\ref{line:optional}, we significantly speed up this search and make the probability of finding only a local minimum instead of the global one higher.
Since all mappings in the orbit are equvialent, such a local minimum will always have the same objective properties $\Theta$ as the real canonical representative (cf. Section~\ref{sec:mapping_problem}).
Thus, finding a local minimum instead of the canonical representative is tantamount to considering a smaller group of symmetries, and thus a very acceptable risk for a considerable speed-up.
Both \mpsym and \mocasin implement this heuristic and use it by default for design-space exploration, as we will see in Section~\ref{sec:dse}. 
We also integrated \mpsym into \mocasin , using the simple Python versions of the algorithms in \mocasin only as fall-back. 

\subsection{Partial Symmetries}
\label{sec:partial_symmetries}

The symmetries we have considered so far can be considered as ``global'' symmetries: they are transformations of the complete structure (e.g. architecture, mapping).
The intuitive notion of symmetry, however, is more general than this.
What we consider as symmetry also includes the relationship of a structure to its parts.
In particular, a symmetry can be local to a part of the structure, without being global. A general discussion of this can be found in~\cite{lawson_inverse_semigroups}, as well as a detailed exposition of the mathematical background of this section.

We can see what we mean by local structures in the example depicted in Figure~\ref{fig:motivation_partial_symmetries}.
It shows two \ac{NoC} architectures both with a regular mesh topology.
The first one is a two-by-two mesh, the second one four-by-four.
We can compare now the symmetries of both architectures intuitively, and see how these translate to the group-theoretic sense.
The four-by-four mesh is larger, and has a sort of self-similarity: it can be thought of as composed of four copies of the two-by-two mesh arranged in a larger two-by-two mesh.
Intuitively, thus, this four-by-four mesh has more symmetry.

\begin{figure}[h]
	\centering
   \resizebox{0.55\textwidth}{!}{\input{figures/placeholder.tex}}
	\caption{A comparison of the symmetries of two meshes.}
	\label{fig:motivation_partial_symmetries}
\end{figure}

However, if we look at the group of automorphisms of the corresponding architecture graphs, we get a result that defies this intuition:
both archtiectures have the same groups of symmetries!
More precisely, their groups of automorphisms are isomorphic, they are dihederal groups on 4 points, $D_4$.
More concretely, there are only 8  possible structure-preserving transformations acting on these two topologies, which are the rotations of $90^\circ,180^\circ,270^\circ,360^\circ = 0^\circ$ and the reflections among each of the axes (horizontal, vertical and both diagonals). We cannot, for example, divide the four-by-four mesh into a two-by-two mesh of two-by-two meshes, and rotate that larger two-by-two mesh by $90^\circ$ or one of the smaller ones by $90^\circ$. These two operations both work locally, if we ignore the rest of the structure, but do not preserve the whole structure of the mesh, as illustrated by Figure~\ref{fig:partial_symmetries_4x4}.

\begin{figure}[h]
	\centering
   \resizebox{0.55\textwidth}{!}{\input{figures/placeholder.tex}}
	\caption{Some partial symmetries of a 4x4 mesh.}
	\label{fig:partial_symmetries_4x4}
\end{figure}

For the mathematical formalization of this intuitive notion of local symmetries, in this section, we follow~\cite{lawson_inverse_semigroups}.
There are essentially two ways equvialent ways of formalizing this intuitive notion of local symmetries, inverse semigroups and ordered groupoids.
We will consider the formalization using inverse semigroups, as it is conceptually simpler for computations, and mathematically equally as powerful.
In the case of global symmetries there are concrete trasformations of architectures and mappings and corrpesponding abstract groups. For partial symmetries we will consider partial trasformations of mappings which we will model as partial permutations, and these partial permutations (transformations) have a corresponding abstract inverse semigroup.

We start by defining partial functions and partial permutations.
\begin{defn}
Let X,Y be sets.
A \emph{partial function} $f: X \rightarrow Y$ is a function from a subset of $X$ to a subset of $Y$.
We denote the domain of by $\dom(f)$ the codomain of $f$ by $\cod(f)$.
Thus, the partial function $f: X \rightarrow Y$ is a (total) function $f: \dom(f) \rightarrow \cod(f)$
\end{defn}

\begin{defn}
Let $X$ be a set.
A partial function $f: X \rightarrow X$ from $X$ to itself is called a \emph{partial permutation} if the (total) function $f: \dom(f) \rightarrow \cod(f)$ is a bijection between $\dom(f)$ and $\cod(f)$.
\end{defn}

\begin{figure}[h]
	\centering
   \resizebox{0.55\textwidth}{!}{\input{figures/placeholder.tex}}
	\caption{An example of a partial permutation in a $4 \times 4$ mesh.}
	\label{fig:example_partial_permutation}
\end{figure}

We can think of partial functions thus as functions that are not defined everywhere, and partial permutations, accordingly, are not defined everywhere.
For example, the partial permutation $f : \{0,\ldots,15\} \rightarrow \{0,\ldots,15\}$ defined as $f(0) = 4, f(1) = 0, f(4) = 5, f(5) = 1$ is a rotation of the upper-left $2 \times 2$-mesh in the $4\times 4$-mesh, but is not defined on the rest of the architecure. This is a partial symmetry of the $4\times 4$-mesh. This partial permutation is dpeicted in Figure~\ref{fig:example_partial_permutation}. We can write it also as:
\begin{equation*}
\left(
\begin{array}{llllllllllllllll}
0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15\\
4 & 0 & - & - & 5 & 1 & - & - & - & - &  - &  - &  - &  - &  - &  -
\end{array}
\right)
\end{equation*}
Because the set $\{0,\ldots,15\}$ is understood from context, we can also write it, shorter, as:
\begin{equation*}
\left(
\begin{array}{llll}
0 & 1 & 4 & 5 \\
4 & 0 &  5 & 1
\end{array}
\right)
\end{equation*}

We also use a notation similar to the cycle notation of group theory, where we use a cicle with round brackets to denote a full cycle, where the last element maps to the first.
Square brackets to denote when this is not the case, i.e. the function is not defined on the last element of that cycle. In this notation, singleton cycles cannot be omitted as in the case of groups.
In other words, fixed points have to be represented as one-element cycles.
The partial permutation from Figure~\ref{fig:example_partial_permutation} can thus be written much more compactly as: $(0,4,5,1)$.
This is a full cycle, but it is only defined on the subset $\{0,1,4,5\}$.
In the group context, the cycle $(0,4,5,1)$ as an element of the symmetric group on 15 points, would instead mean the (complete) permutation that fixes $\{2,3,6,7,8,9,10,11,12,13,14,15\}$.
As a partial permutation in cycle notation we would write this as: \[(0,4,5,1)(2)(3)(6)(7)(8)(9)(10)(11)(12)(13)(14)(15) \]
A different example of a partial permutation is the partial permutation that moves the first row to the right (and is not defined on the rest), as a cycle: $[0,1][4,5][8,9][12,13]$.
On the other hand, the partial permutation that is a diagonal reflection on the upper-left $2 \times 2$ (sub)mesh is, in cycle notation, $(1,4)(0)(5)$.
These two partial permutations can also be written in the matrix notation from above as:

\begin{equation*}
\left(
\begin{array}{llll}
0 & 4 & 8 & 12 \\
1 & 5 & 9 & 13 
\end{array}
\right)
\quad
\left(
\begin{array}{llll}
0 & 1 & 4 & 5 \\
0 & 4 & 1 & 5
\end{array}
\right)
\end{equation*}

For computations~\cite{east2019semigroups}, the three notations can be interpreted to make different data structures that make different opperations more efficient, like application of the partial function (as an array look-up), for sparse partial permutations (as lookups in key-value pairs), or cycles for efficient multiplication (as concatenation). They have different benefits and drawbacks. For readability though, the cycle notation is the most compact one, and the one we will use for the rest of this thesis.

Just as for groups, we can define the (left) action of a semigroup:
\begin{defn}
Let $S$ be a semigroup and $X$ be a set. We say that $S$ \emph{acts} on $X$ (on the left) if there is a function $\cdot : S \times X \rightarrow X$ such that $(ab) \cdot x = a \cdot (b \cdot x)$. If $S$ is a monoid with identity $1$ and the function $\cdot$ satisfies the condition $1 \cdot x = x$ for all $x \in X$, we say that the action is a monoid action.
\end{defn}

The action of a semigroup of partial permutations to an architecture works the same as with groups, except it does not work on the whole architecture.
Let $f$ be a partial permutation on an architecture $A$, and $m : T \rightarrow A$ be a mapping on that arcticeture.
If the partial permutation is defined on all cores mappend on by $m$, i.e., $im(m) \subseteq \dom(f)$, then we can use the action of the semigroup of partial permutations of $A$ to define another mapping $fm$ by $fm(t) = f \cdot m(t)$
for all $t$ in $T$. If $f$ is not defined on some of the cores of $m$, i.e., $im(m) \not \subseteq \dom(f)$, then we cannot define $fm$. In this way, $f$ also defines a partial permutation $\hat f$ on the set of mappings $\{ m : T \rightarrow A \}$.
\todo{check that notation is consistent}

Consider for example the mapping of an application with three tasks to the $4 \times 4$-mesh defined by $m_0(t_0) = m_0(t_2) = \PE_0$ and $m_0(t_1) = \PE_1$, which we can also write as the vector $m_0 = (0,1,0)$.
Then the partial permutation $(0,4,5,1)$ from Figure~\ref{fig:exapmle_partial_permutation} above defines the mapping $(0,4,5,1)m = (4,0,4)$.
Similarly, the action of the partial permutation $(1,4)(0)(5)$ yields a new mapping, $(1,4)(0)(5)m_0 = (0,4,0)$.
However, since the translation $[0,1][4,5][8,9][12,13]$ is not defined on $1 = m_0(t_1)$, we cannot define $[0,1][4,5][8,9][12,13]$ as a mapping.
Formally we can say that the partial permutations $\widehat{(0,4,5,1)}$ and $\widehat{(1,4)(0)(5)}$ are defined on $m_0$, but $\widehat{[0,1][4,5][8,9][12,13]}$ is not defined on $m_0$.

What happens with application symmetries? Consider ...

Define orbits for inverse semigroups and s.c.c.

We are now ready to formally define the set of partial symmetries of architectures, applications in mappings as in the case of groups.
\begin{defn}
$\AutSemi$
\end{defn}

There is a different way of intperpreting these notions of symmetry, using graph isomorphisms.
Recall that a mapping $m: T \rightarrow A$ can be seen as a morphism of graphs from $T$ to $A$.
In particular, every mapping $m$ defines a subgraph $m(T) \leq A$.
This subraph has a node $m(t) \in V_A$ for every $PE$ in the architecture $A$ that is used in a mapping, and similarly an endge $(m(t_1),m(t_2)) \in E_A$ for every communication primitive where a channel is mapped to.
Figure~\ref{fig:example_graph_isomorphism} shows this graph for two of the mappings discussed above, $m_0 = (0,1,0)$ and $m_1 := (0,4,5,1)m_0 = (4,0,4)$. 
You can see that the architecture subgraphs of the mappings are identical. In fact, if there is a partial symmetry, these two graphs will always be identical:

\begin{figure}[h]
	\centering
   \resizebox{0.55\textwidth}{!}{\input{figures/placeholder.tex}}
	\caption{Some partial symmetries of a 4x4 mesh.}
	\label{fig:partial_symmetries_4x4}
\end{figure}
It is a sort of trace of the mapping. This mapping does not describe 
\todo{check that notation is consistent}

\begin{lem}
\label{lem:symmetry_to_iso}
Let $m : T \rightarrow A$ be a mapping and let $f \in \AutSemi(A)$ be a partial automorphism of the archtitecture such that $\im(m) \subseteq \dom(f)$. Then, the two graphs $m(T)$ and $(fm)(T)$ are isomoprhic and the function $\varphi: m(T) \rightarrow (fm)(T), m(t) \mapsto f \cdot m(t)$ for all $t \in T$ is an isomorphism of labeled graphs.
\begin{proof}
First note that $\varphi$ is well-defined. Indeed, since $\im(m) \subseteq \dom(f)$ it means that $f \cdot m(t)$ is defined for all $t \in T$. Since $f \in \AutSemi(A)$, we know that the type of $m(t)$ and $f \cdot m(t)$ is equal for all $t \in T$, as well as the type of all edges $(m(t_0),m(t_1))$ and  $(f \cdot m(t_0), f \cdot m(t_1))$ is equal. Thus, $\varphi$ is a morphism of labeled graphs. Finally, since $f \in \AutSemi(A)$, we know that $f$ is a partial permutation, and in particular, a bijection between $\dom(f)$ and $\cod(f)$. In particular, $\varphi$ is bijective, and as a bijective morphism of labeled graphs, an isomorphism. 
\end{proof}
\end{lem}

What about the converse, if the subgraphs generated by the mappings are isomorphic, does this mean that there is a (partial) isomorphism of the mappings too? Can we use this to characterize equivalent mappings? 
In general, no.
Consider the subgraph of the mappings $m_2 := (4,4,0)$ and $m_3 := (4,0,0)$. Both these mappings project into isomorphic subgraphs $m_2(T) \cong m_3(T) \cong m_0(T)$, but obviously the mappings are not equivalent.
Even if the subgraphs are isomorphic, the crucial difference is, however, that the mapping $\varphi$ as defined in Lemma~\ref{lem:symmetry_to_iso} is \textbf{not} an isomorphism of (labeled) graphs. 
What if tasks $t_1$ and $t_2$ are equivalent?
In other words,  what if $g = (0)(1,2)$ is a (full) automorphism of the application graph?
Then the mappings $m_0$ and $m_2$ are equivalent (via $g$), but the function $\varphi$ of Lemma~\ref{lem:symmetry_to_iso} is still not an isomoprhism of the subgraphs.
However, we can generalize the function by applying $g$ first, as $\varphi \circ g : m(T) \rightarrow fm(T), m(t) \mapsto (fm \circ g)(t) = (fm)(g(t))$.
This generalization, in fact, yields a full characterization of equvialent mappings through isomorphy of subgraphs.
\begin{theorem}
Let $A$ be an architecture with inverse semigroup of automorphisms $S = \AutSemi(A)$ and let $T$ be an application graph with group of automorphsims $G = \Aut(T)$.
For mappings $m,m' : T \rightarrow A$, the following statements are equvialent:
\begin{enumerate}
\item There exists a partial permutation $f \in S$ and a perumtation $g \in G$, such that $\varphi \circ g$ is an isomorphism of labeled graphs.
\item There two mappings are equivalent by symmetries in the orbit of $S \times G$.
\end{enumerate}
\begin{proof}
The implication $(1) \Rightarrow (2)$ follows directly from the definition of $\varphi$ and the action of $S \times G$.
For the implication $(2) \Rightarrow (1)$, since $m$ and $m'$ are in the same s.c.c. of the orbit of $S \times G$, there exists an $x \in S \times G$ such that $m = x \cdot m'$.
We can use the direct product structure of $S \times G$ to decompose $ x = fg$ for $f \in S, g \in G$.
This means that $m = fg \cdot m' = f \cdot (g \cdot m')$.
Applying Lemma~\ref{lem:symmetry_to_iso} on $m$ and $g \cdot m'$ shows that $\varphi \circ g$ is an isomorphism.
\end{proof}
\end{theorem}

How do partial symmetries with inverse semigroups compare to (global) symmetries, in the sense of group theory?
We can start with a simple example, of a $2\times 2$ mesh, which we will call $M_2$. The group of symmetries of this architecture, as we have seen, is $D_4$ with $|D_4| = 8$ symmetries.
What about the partial symmetries? It is easy to check that $|\AutSemi(M_2)| = 45$, which is a lot more partial symmetries than global ones! But in fact, comparing the size of the group and the semigroup is misleading.
We can't compare them, as they deal with different objects, functions vs partial functions. For this case of $M_2$, in a sense, we do not get any more symmetries by going to the partial symmetry world.
We can see it through the following argument: the group $\Aut(M_2) \cong D_4$ acts canonically on the power set of $M_2$, $\operatorname{Pow}(M_2)$, simply by acting element-wise:
For $M \subseteq M_2$ and $g \in \Aut(M_2)$, the (canonical) action is defined as follows: $g \cdot M := \{ g \cdot m \mid m \in M \}$.
In this action, the orbits $G \\ \operatorname{Pow}(M_2)$ are in obvious bijection to the s.c.c. of the orbit of $\AutSemi(M_2) \\ \operatorname{M_2}$.

We have seen how to describe partial symmetries, a natural question is how how to calculate them?
This can be accomplished with the methods of~\cite{east2019semigroups}, and our applications of it in joint work with S. Siccha and J. Castrillon~\cite{goens_taco17}.
In fact, \mpsym implements Algorithm~2 from~\cite{goens_taco17}.
We worked with Sebastian Krammer in his bachelor thesis~\cite{krammer_bachelor} on finding more efficient algorithms. 
Unfortunately, the algorithms as implemented so far are not efficient enough to be useful in the context of mappings and \ac{dse}.

In future work, we believe we should be able to find explicit generating systems for an $n \times n$ mesh for an arbitrary $n$, which would significantly improve the performance of the algorithms, which is limited by finding a good generating set.
Using inverse semigroups also opens up an additional avenue for future work, where \emph{similarities} can be described instead of precise symmetries. \index{mapping similarities}
For example, mapping an edge between two cores in a mesh to a different edge type with a \emph{smaller} number of hops is sure to not worsen the performance of the application, although we cannot say if it will improve it or not.
Such a transformation can also be described with semigroups, and the directed graph structure of the orbits nicely encompasses such one-way transformations.
