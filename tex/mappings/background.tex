In software synthesis we strive to derive an efficient execution from a computation expressed in an abstract model to a concrete architecture.
In order to understand software synthesis, thus, we first need to understand the underlying models of computation.
Closely related to these concurrent models of computation are execution traces, which capture a concrete execution for some input.
However, going from such abstract models to a concrete execution also requires an understanding of the target hardware architecture.
The relationship between the two can be captured in a mapping.
This chapter considers all of these aspects with their corresponding models, and the relationship between them.
It is the central piece of background theory required for the methods presented in this thesis.

\subsection{Kahn Process Networks}

The flow we investigate in Part I is based on the \ac{MoC} of \acfp{KPN}.
In this section we introduce this model, or rather, its most common implementation with blocking-read semantics~\cite{kahn_macqueen} .
In Part II, specifically in Chapter~\ref{chap:mocs} we will discuss the original formal semantics~\cite{kahn74} and how they differ to those introduce here.
We also discuss other models of computation and how they relate to each other.

We can think of a \ac{KPN} as a computation distributed among different \emph{processes}\index{KPN ! process} (originally derived from coroutines).
Each of these processes executes sequentially and is Turing complete. However, the processes share no memory, they have memories local only to themselves.
They communicate between each other using \emph{channels}\index{KPN ! channel}, which work as unbounded \acs{FIFO} buffers. 
Processes have sets of outgoing and incoming channels.
As an instruction, any process can write to one of its outgoing channels or read from one of its incoming channels.
They do so in discrete tokens of data.

The original language~\cite{kahn_macqueen} was proposed as an extension of POP-2, which is pretty dated and has fallen out of use today.
Instead of this language, we will consider a more modern incarnation, \ac{CPN}, which extends the C programming language~\cite{cpn}\index{CPN}.
We do so by looking at the example from Listing~\ref{listing:fft_cpn}. 
The listing shows a very simplified process that applies an \acs{FFT} it the data in its incoming channel and outputs it to an outgoing channel.
Processes in \ac{CPN} are instantiated from process templates, similar to classes and objects in object-oriented languages.


\begin{listing}
\begin{minted}{C}
__PNkpn fft_templ
    __PNin(param_t param, time_coef[N])
    __PNout(fp_complex freq_coef[N]){
  int i, loop_count;
  __PNin(param){
    loop_count = param.loop_count;
  }
  for (i = 0; i < loop_cnt; i++)
    __PNin(time_coef) __PNout(freq_coef){
      fft(time_coef, freq_coef);
    }
}
\end{minted}
\caption{An \ac{FFT} implemented as a \AC{KPN} process in \ac{CPN}}
\label{listing:fft_cpn}
\end{listing}


The communication is asynchronous: 
When a process writes to an outgoing channel, the data is buffered in the channel until it is read, and the process continuous to execute.
If a process reads from a channel, it receives the oldest token buffered in the channel.
If there are no tokens, execution blocks until such a token is written to a channel - hence the name, blocking-read semantics.
A channels can be the outgoing channel of at most one process (it should also be so for at least one process, otherwise the channel is useless).
On the other hand, if a channel is an incoming channel to multiple processes, all tokens are copied for each of those processes.
Hence, all processes will see the exact same incoming stream of tokens from a shared channel, instead of splitting them up.

Let us consider the \acs{FFT} process from Listing~\ref{listing:fft_cpn} and combine it with other processes into a full application.
Listing~\ref{listing:audio_filter} describes a simplified algorithm for a low-pass filter on a stereo sound file, using this \acs{FFT} process.
We also omit the templates and channel declarations in this simplified listing.
The \texttt{src} process reads the stereo file, splits it into two channels and sends the sound in blocks of a determined length as tokens.
These files are then transformed from the time domain to the frequency domain using a \acf{FFT}, filtered and transformed back to the time domain.
A sink channel gathers the filtered blocks from both channels, left and right, and combines them again into a stereo sound file that it can store.

\begin{listing}
\begin{minted}{C}
__PNprocess src = src_templ
  __PNout(param_chan, src_l, src_r);
__PNprocess fft_l = fft_templ
  __PNin(param_chan, src_l)
  __PNout(fft_coef_l);
__PNprocess filter_l = filter_templ
  __PNin(param_chan, fft_coef_l)
  __PNout(filtered_coef_l);
__PNprocess ifft_l = ifft_templ
  __PNin(param_chan, filtered_coef_l)
  __PNout(sink_l);
__PNprocess fft_r = fft_templ
  __PNin(param_chan, src_r)
  __PNout(fft_coef_r);
__PNprocess filter_r = filter_templ
  __PNin(param_chan, fft_coef_r)
  __PNout(filtered_coef_r);
__PNprocess ifft_r = ifft_templ
  __PNin(param_chan, filtered_coef_r)
  __PNout(sink_r);
__PNprocess sink = sink_templ
  __PNin(param_chan, sink_l, sink_r);
\end{minted}
\caption{An audio filter \AC{KPN} application in \ac{CPN}}
\label{listing:audio_filter}
\end{listing}

The data flow in the example of Listing~\ref{listing:audio_filter} is very structured: it goes from the source, split over both channels, through the filter, back to the sink.
This structure can easily be visualized in a graph, like in Figure~\ref{fig:audio_filter_graph}.
More generally, we can think of any \ac{KPN} application as a directed graph $K = (T,C)$, where the nodes $T$ represent the processes\footnote{the notation comes from $T$ for tasks}, and the edges $C$, the channels.
This works even when a channel is an incoming channel for multiple processes.
In that case, we can split it into multiple edges from the process it is going from, to each of the target channels.
We can do so without loss of generality since these are the semantics of such channels. 
We call this graph the \emph{\ac{KPN} graph}\index{KPN ! graph}.

\begin{figure}[h]
	\centering
   \resizebox{0.9\textwidth}{!}{\input{figures/placeholder}}
   \caption{The audio filter application as a \ac{KPN} graph}
	\label{fig:audio_filter_graph}
\end{figure}

\subsection{Execution Traces}

Kahn Process Networks with mathematical semantics~\cite{kahn74}, in the sense of Scott~\cite{scott1970}, abstract away the concrete implementation of individual steps in a computation.
Even so, the execution of a computation can be thought of as a series of steps or partial computations that eventually yield the final result.
These series, which is commonly referred to as execution trace, can be captured as a sequence of steps, e.g. as the element of a Scott Domain\footnote{this will be discuss more in-depth in Chapter~\ref{chap:mocs}}. 
Abstract computations, modeled as Scott-continuous functions, can can make computations of arbitrary length.
This is modeled by (countably) infinite sequences in $D^\omega \setminus D^*$.
A concrete execution, on the other hand, always has a finite length.
It always resides in $D^*$.
For a (Scott-continuous) function, this sequence can be modeled as a finite string in the computation domain.

In a concurrent execution, multiple entities concurrently execute steps.
As modeled by Kahn, these entities all implement individual (Scott-continuous) functions.
As such, there is not a unique series of steps that can be said to be the execution trace of the computation.
To see this, consider the following example:
\begin{ex}
  Todo
\end{ex}

TODO: discuss example

In the distributed case thus, the execution traces are in fact equivalence classes of strings.
We define this more formally, following~\cite{mazurkiewicz1995introduction}, the first chapter of~\cite{diekert1995book}.
Let $D$ be a finite set, like an alphabet $\Sigma$ or data type $D = \Cup_{i \in \mathcal{I}} D_i$ be a data type for a \ac{KPN}.
Let $\Delta$ be a symmetric, reflexive relation on $D$, which we call a dependency\index{dependency}.
This means that if $(a,b) \in \Delta$, we have $(b,a) \in \Delta$ and also $(a,a) \in Delta$ for all $a \in D$. 
With $\Delta$ we define an additional relation in $D$, namely $I := (D \times D) \setminus \Delta$.
We call $I$ the induced independency\index{independency}. 
We define an equivalence relation $\sim_I$ on the monoid $D^*$ (with respect to concatenation) as follows:
We require for $a,b \in I$ then $ab \sim_I ba$. The relation $\sim_I$ is defined as the least congruence that satisfies this requirement.
Note that a congruence is an equivalence relation that respects the algebraic structure, in this case the monoid structure of the concatenation operation.
We call the equivalence classes of $D^*/{\sim_I}$ traces. 
By definition, the concatenation operation on $D^*$ factors over the equivalence relation $\sim_I$,
and thus $D^*/{\sim_I}$ defines a monoid (with identity element $[\epsilon]_{\sim_i}$, where $\epsilon \in D^*$ is the empty string).
We call this the Trace Monoid\index{trace monoid}, $T(D)$.
We care about the algebraic structure of a monoid since it is central to the definition of Scott-continuity.

\begin{ex}
  Todo (based on example above)
\end{ex}

There are two additional equivalent definitions of this monoid as histories and dependence graphs.
We present histories here, as they are easier for the intuition.
Instead of a single alphabet (or set of types) $D$, we have a finite set of alphabets $D := (D_i), i \in \mathcal{I}$, where $\mathcal{I}$ is a finite index set.
We can think of the indices as corresponding to the processes or actors in the system, and the alphabets $D_i$ to the alphabets of these individual entities.
If we think of the individual entities as computing some (Scott-continuous) function, their execution trace will be a unique string $a_i \in D_i^*$ (recall that concrete executions are finite).
Since, in general, these entities do not compute independently, they have common synchronization points.
These synchronization points are abstractly modeled in the computation alphabet by mutual elements in $D_i \cap D_j$ for two entities $i,j \in \mathcal{I}$.
We can define a monoid, the product monoid~\index{product monoid} $P(D)$, by component-wise concatenation of the strings: $(a_i)_i (b_i)_i = (a_ib_i)_i$ for all $i \in \mathcal{I}$.
However, not every such a string product can be the history of a system.
The synchronization points of different subsystems should be consistent with each other. Consider the following example:
\begin{ex}
  \label{ex:inconsistency_history}
TODO: slight variation that shows an element of $P(D) \setminus H(D)$
\end{ex}
To avoid this, we want to ensure histories are consistent. For this, we define elementary histories~\index{elementary history} as follows:
For any $a in \Cup_{i \in \mathbb{I}} D_i$, the elementary history of $a$ is the tuple $(a_i)_{i \in \mathbb{I}}$, with $a_i = \left\{ \begin{array}{ll} a, & \text{ if } a \in D_i, \\ \epsilon, & \text{ otherwise. }\end{array}\right.$
Again, $\epsilon$ represents the empty string.
The monoid generated by all elementary histories for elements in $\Cup_{i \in \mathbb{I}} D_i$ is called the history monoid $H(D)$, and is a submonoid of $P(D)$.
If we examine the definition, it is not difficult to convince ourselves that these are precisely the histories which avoid inconsistencies like those of Example~\ref{ex:inconsistency_history}.

\begin{ex}
  Todo (based on example above)
\end{ex}

We can go from a trace to a history by the morphism $\pi: T(\Cup_{i \in \mathcal{I}}) \rightarrow H(D), a \mapsto (\pi_i(a))_i, i \in \mathcal{I}$, where $\pi_i$ is the projection $\Cup_{i \in \mathcal{I}}D_i \rightarrow D_i$.
Here, for the trace monoid $T(D)$ we define the dependencies to be $\Cup_{i \in \mathcal{I}}D_i \times D_i$. 
This is not just a morphism, but in fact an isomorphism: See Theorem 1.5.4 of~\cite{mazurkiewicz1995introduction}.
Thus, the two concepts are equivalent.
For the rest of this thesis we will use the terms traces and histories interchangeably.


Traces, and equivalently histories, can be used to describe the concrete computations in concurrent systems like those described by a \ac{KPN}.
They are also well-suited (and well-suited) to model these systems in the context of process calculi, like \ac{CSP}.
However, an important observation is the converse: a concrete execution of a \ac{KPN} is determined uniquely by its history.
Moreover, any concrete implementation of the \ac{KPN} realizing the same execution will have the same history: the history is an invariant of the abstract execution model.
It captures the concurrent essence of the concrete computation.

\subsection{Architecture Models}
Discuss ad-hoc models, \cite{pelcat2015models}. Go in-depth on architecture graph (and topology graph).

\subsection{The Mapping Problem}
A lot more...\cite{singh2013mapping} \cite{marwedel2011mapping}

\subsection{Limits of the Model}
Only discuss briefly here.
