At the basis of software synthesis are concurrent models of computation. The process derives an efficient execution to a concrete architecture from a computation expressed in an abstract model.
In order to understand software synthesis thus, we first need to understand the underlying models of computation.
Closely related to these concurrent models of computation are execution traces, which capture a concrete execution for some input.
However, going from such abstract models to a concrete execution also requires an understanding of the target hardware architecture.
The relationship between the two can be captured in a mapping.
This chapter considers all of these aspects with their corresponding models, and the relationship between them.
It is the central piece of background theory required for the methods presented in this thesis.

\subsection{Kahn Process Networks}

Models of computation exist at basically every level of abstraction and usefulness. In this first part of the thesis, we will focus on a single model, Kahn Process Networks.
In Part II, specifically in Chapter~\ref{chap:mocs} we will discuss other models and how they relate to each other.


In this chapter we will introduce Kahn Process Networks only superficially. We will 
\subsection{Execution Traces}
Kahn Process Networks with mathematical semantics~\cite{kahn74}, in the sense of Scott~\cite{scott1970}, abstract away the concrete implementation of individual steps in a computation.
Even so, the execution of a computation can be thought of as a series of steps or partial computations that eventually yield the final result.
These series, which is commonly referred to as execution trace, can be captured as a sequence of steps, e.g. as the element of a Scott Domain\footnote{this will be discuss more in-depth in Chapter~\ref{chap:mocs}}. 
Abstract computations, modeled as Scott-continuous functions, can can make computations of arbitrary length.
This is modeled by (countably) infinite sequences in $D^\omega \setminus D^*$.
A concrete execution, on the other hand, always has a finite length.
It always resides in $D^*$.
For a (Scott-continuous) function, this sequence can be modeled as a finite string in the computation domain.

In a concurrent execution, multiple entities concurrently execute steps.
As modeled by Kahn, these entities all implement individual (Scott-continuous) functions.
As such, there is not a unique series of steps that can be said to be the execution trace of the computation.
To see this, consider the following example:
\begin{ex}
  Todo
\end{ex}

TODO: discuss example

In the distributed case thus, the execution traces are in fact equivalence classes of strings.
We define this more formally, following~\cite{mazurkiewicz1995introduction}, the first chapter of~\cite{diekert1995book}.
Let $D$ be a finite set, like an alphabet $\Sigma$ or data type $D = \Cup_{i \in \mathcal{I}} D_i$ be a data type for a \ac{KPN}.
Let $\Delta$ be a symmetric, reflexive relation on $D$, which we call a dependency\index{dependency}.
This means that if $(a,b) \in \Delta$, we have $(b,a) \in \Delta$ and also $(a,a) \in Delta$ for all $a \in D$. 
With $\Delta$ we define an additional relation in $D$, namely $I := (D \times D) \setminus \Delta$.
We call $I$ the induced independency\index{independency}. 
We define an equivalence relation $\sim_I$ on the monoid $D^*$ (with respect to concatenation) as follows:
We require for $a,b \in I$ then $ab \sim_I ba$. The relation $\sim_I$ is defined as the least congruence that satisfies this requirement.
Note that a congruence is an equivalence relation that respects the algebraic structure, in this case the monoid structure of the concatenation operation.
We call the equivalence classes of $D^*/{\sim_I}$ traces. 
By definition, the concatenation operation on $D^*$ factors over the equivalence relation $\sim_I$,
and thus $D^*/{\sim_I}$ defines a monoid (with identity element $[\epsilon]_{\sim_i}$, where $\epsilon \in D^*$ is the empty string).
We call this the Trace Monoid\index{trace monoid}, $T(D)$.
We care about the algebraic structure of a monoid since it is central to the definition of Scott-continuity.

\begin{ex}
  Todo (based on example above)
\end{ex}

There are two additional equivalent definitions of this monoid as histories and dependence graphs.
We present histories here, as they are easier for the intuition.
Instead of a single alphabet (or set of types) $D$, we have a finite set of alphabets $D := (D_i), i \in \mathcal{I}$, where $\mathcal{I}$ is a finite index set.
We can think of the indices as corresponding to the processes or actors in the system, and the alphabets $D_i$ to the alphabets of these individual entities.
If we think of the individual entities as computing some (Scott-continuous) function, their execution trace will be a unique string $a_i \in D_i^*$ (recall that concrete executions are finite).
Since, in general, these entities do not compute independently, they have common synchronization points.
These synchronization points are abstractly modeled in the computation alphabet by mutual elements in $D_i \cap D_j$ for two entities $i,j \in \mathcal{I}$.
We can define a monoid, the product monoid~\index{product monoid} $P(D)$, by component-wise concatenation of the strings: $(a_i)_i (b_i)_i = (a_ib_i)_i$ for all $i \in \mathcal{I}$.
However, not every such a string product can be the history of a system.
The synchronization points of different subsystems should be consistent with each other. Consider the following example:
\begin{ex}
  \label{ex:inconsistency_history}
TODO: slight variation that shows an element of $P(D) \setminus H(D)$
\end{ex}
To avoid this, we want to ensure histories are consistent. For this, we define elementary histories~\index{elementary history} as follows:
For any $a in \Cup_{i \in \mathbb{I}} D_i$, the elementary history of $a$ is the tuple $(a_i)_{i \in \mathbb{I}}$, with $a_i = \left\{ \begin{array}{ll} a, & \text{ if } a \in D_i, \\ \epsilon, & \text{ otherwise. }\end{array}\right.$
Again, $\epsilon$ represents the empty string.
The monoid generated by all elementary histories for elements in $\Cup_{i \in \mathbb{I}} D_i$ is called the history monoid $H(D)$, and is a submonoid of $P(D)$.
If we examine the definition, it is not difficult to convince ourselves that these are precisely the histories which avoid inconsistencies like those of Example~\ref{ex:inconsistency_history}.

\begin{ex}
  Todo (based on example above)
\end{ex}

We can go from a trace to a history by the morphism $\pi: T(\Cup_{i \in \mathcal{I}}) \rightarrow H(D), a \mapsto (\pi_i(a))_i, i \in \mathcal{I}$, where $\pi_i$ is the projection $\Cup_{i \in \mathcal{I}}D_i \rightarrow D_i$.
Here, for the trace monoid $T(D)$ we define the dependencies to be $\Cup_{i \in \mathcal{I}}D_i \times D_i$. 
This is not just a morphism, but in fact an isomorphism: See Theorem 1.5.4 of~\cite{mazurkiewicz1995introduction}.
Thus, the two concepts are equivalent.
For the rest of this thesis we will use the terms traces and histories interchangeably.


Traces, and equivalently histories, can be used to describe the concrete computations in concurrent systems like those described by a \ac{KPN}.
They are also well-suited (and well-suited) to model these systems in the context of process calculi, like \ac{CSP}.
However, an important observation is the converse: a concrete execution of a \ac{KPN} is determined uniquely by its history.
Moreover, any concrete implementation of the \ac{KPN} realizing the same execution will have the same history: the history is an invariant of the abstract execution model.
It captures the concurrent essence of the concrete computation.

\subsection{Architecture Models}
Discuss ad-hoc models, \cite{pelcat2015models}. Go in-depth on architecture graph (and topology graph).

\subsection{The Mapping Problem}
A lot more...\cite{singh2013mapping} \cite{marwedel2011mapping}

\subsection{Limits of the Model}
Only discuss briefly here.
