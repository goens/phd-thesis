This section will survey some of the most important concurrent models of computation. Before diving into the models, we will first discuss the mathematical semantics\footnote{Nowadays we call these semantics denotational} of computation by Scott.

\subsection{Partial Computation: Scott Domains}

When Scott proposed his mathematical theory of computation~\cite{scott1970}, he used the term mathematical to contrast it with operational computation.
In practice, the steps of a computation are defined by the \ac{ISA} of the machine executing them.
Most people don't write programs directly for the \ac{ISA}, however. They write them in an abstract programming language, which is translated by a compiler into machine instructions.
Thus, in practice, the implementation of a compiler is what defines the (operational) semantics of programs.
Scott's theory had the ambitious goal of being an abstraction that sat between these operational semantics and the abstract notions of computability of e.g. Church or Turing.
He intended to abstract away the arbitrary implementation choices that were necessary but did not change the essence of the execution.
While today his model is not the single established abstract model of semantics he sought out to define, it introduced several important ideas and mathematical structures to models of computation.
In particular, a crucial abstraction introduced by his theory is that of partial computation.
His theory makes it possible to express a computation as a series of partial results, without regarding the actual implementation of these.
We will now introduce the basics of Scott's mathematical theory of computation.

Two related concepts can be used to computation in Scott's semantics, $\omega$-complete partial orders~\cite{gunter} or complete semi-lattices~\cite{lee_matsikoudis_semantics}. \index{$\omega$-complete partial order}
We will use the latter.
Let $\langle A, \leq \rangle$ be a \ac{poset}. For a subset $B \subseteq A$, we say $a$ is an upper or lower bound of $B$ if $a \geq b$ (resp. $\leq$) for all $b \in B$. 
Similarly, we say $a$ is a \emph{greatest lower bound}/\emph{least upper bound} of $B$ if $a$ is a lower/upper bound of $B$ and for all other lower/upper bounds $a'$ we have $a \leq/ b$ or $a \geq b$, respectively.\index{(greatest) lower bound}\index{(least) upper bound}
A nonempty set $D \subseteq A$ is then called \emph{directed} if every nonempty subset of $D$ has an upper bound \index{directed set}. If every such set $D$ has a least upper bound, we say that $A$ is directed-complete.
In that case, we denote the least upper bound of $D$ as $\sqcup D$.
If $A$ additionally has a least element $\bot \in A$ with $\bot \leq a$ for all $a \in A$, we say that $A$ is a complete partial order.
If, instead, $A$ is directed-complete and every non-empty subset has a greatest lower bound, we say $A$ is a complete semilattice.\index{complete semilattice}

The canonical example of this are sequences, which are a generalization of strings. Let $\Sigma$ be an alphabet (a set). 
We call $\Sigma^*$ the set of words (Kleene star) over $\Sigma$, and $\Sigma^\omega = \mathbb{N} \rightarrow \Sigma$ is the set of (countably) infinite sequences over $\Sigma$.\index{Kleene star}
We then define $S = \Sigma^* \cup \Sigma^\omega$ as the set of (finite or infinite) sequences over the alphabet $\Sigma$.\index{sequences}
The set of sequences $S$ is obviously a \ac{poset} with the prefix relation $\sqsubseteq$, where $s \sqsubseteq s'$ iff there exists a $t \in S$ with $s.t = s'$.
Here, $(.) : S \rightarrow S$ denotes the concatenation operator (which coincidentally makes $S$ a monoid with neutral element $\epsilon$, the empty string).
In fact, $S$ is a complete semilattice with regards to $\sqsubseteq$ (cf.~\cite{lee_matsikoudis_semantics}).
In Scott's model, these sequences describe the partial steps of a computation process, generating data in discrete steps (not necessarily all at once). 

A function $f : S \rightarrow S$ is called monotone if for $s \sqsubseteq s'$ it holds that $f(s) \sqsubseteq f(s')$.\index{Scott ! monotone function}
Interpreting $f$ as computation, this models causality: having more input data cannot change the data that has already been output.
In other words, the future cannot change the past.
An additional, more technical definition is that of continuity.\index{Scott ! continuity}
A monotone function $f : S \rightarrow S$ is called continuous if for all directed sets $D$ in $S$, it holds that $f (\sqcup D) = \sqcup f(D) := \sqcup \{ f(s) \mid s \in D \}$.
This concept is distinct from that of a monotone function only for infinite sequences.
It means that a function will not produce its output only after reading an infinite amount of input.
We call this continuous because the prefix relation defines a topology on the set $S$, the Scott topology.\index{Scott ! topology}

\subsection{Concurrent Computation}
\label{sec:concurrent_mocs}
Scott's computation model implicitly assumed a sequential computation process, and Scott-continuous functions are a powerful method for describing partial sequential computations.
Can we also use this model to describe parallel computation?
Gilles Kahn did precisely this, four years after Scott published his mathematical theory of computation. 
He used the formalism of Scott to define a model of parallel computation, based on what he coined as process networks, now known as \acfp{KPN}\cite{kahn74}.

The basic idea to generalize the Scott theory of computation for concurrent execution is simple.
We compose functions in networks of Scott functions, these are the \acp{KPN}.
These composed functions yield a system of equations.
For example, we can compose a Scott continuous function $f$ with itself by applying it to its output.
This yields an equation: $f(s) = f(f(s))$, which is solved by a fixed point of $f$ (i.e. a sequence $s \in S$ with $f(s)= s$).
A series of related results on such systems of equations and fixed-points by Tarski, Kleene and others show that such a system always has a least fixed point.
This defines the semantics of \ac{KPN}.
For example, for the case of the single function $f$ as above, if $f$ is the identity function, this least fixed point is $\epsilon$.
This solves problems with loops in the system by giving well-defined semantics, and even yields a procedure to find the fixed points, by recursively applying the functions.
In particular, this means that \ac{KPN} are deterministic (as per their fixed-point semantics).

There are other related models that span from the same time period, like the Hewitt-Agha actor model\index{actor model}~\cite{DBLP:conf/ijcai/HewittBS73,Agha:86:Actors}.
This was also a model of parallell computation. In it, actors communicate with other actors via messages in a non-deterministic fashion. 
Actors can also be dynamically created and the connections between them are also dynamic.
While this yields much more flexibility, it comes with a high price: deteriminism. 

Other models of parallel computation include Petri Nets\index{Petri nets}~\cite{petri1962nets}, in which a bipartite graph of places and transitions models the distributed execution of a system.
Transitions in petri nets are very flexible as well, but they are also non-deterministic, the order in which multiple activated transitions fire is non-deterministic in general.

A series of more abstract models are the Process Calculi\index{Process Calculi}, which includes the well-known $\Pi$-calculus\index{Process Calculi ! Pi-Calculus} and \ac{CSP}.\index{Process Calculi ! CSP}
These models are called calculi because they define specific composition rules, like parallel composition $A | B$ or $A \| B$ for processes with clear semantics. 
They are well-known for describing systems and specifying their behavior, e.g. in the context of model checking~\cite{baier_model_checking}.
Technically, however, these are also very abstract models of computation.

Figure~\ref{fig:mocs_overview} shows an overview of the different models of computation and their properties.
The dotted nodes refer to abstract properties of the models, whereas the filled nodes are concrete models.
Concretely, the ones colored blue are that we review and use more in detail in this thesis.
Timed models, like reactors, will be discussed in Section~\ref{sec:reactors}, and dataflow models in the section below.
This figure was inspired by Figure~1.6 in~\cite{Ptolemaeus:14:SystemDesign}.

\begin{figure}[h]
	\centering
   \resizebox{1.00\textwidth}{!}{\input{figures/mocs.tex}}
	\caption{Overview of different models of computation. Color-filled nodes refer to concrete models, dotted ones are abstract properties. }
	\label{fig:mocs_overview}
\end{figure}


\subsection{Dataflow Models of Computation}

A series of models stands out in the context of software synthesis and also in the domain of embedded system software, these are dataflow models of computation.
More dataflow models have been proposed than what we could reasonably list and describe here.
The original idea however, or at least one of the first to be published, goes back to Dennis~\cite{dennis1974first,dennis1986data}\index{Dataflow ! Dennis}
These dataflow models were also related with \ac{KPN}, in so-called dataflow process networks~\cite{lee1995dataflow,lee_matsikoudis_semantics}
Common among most dataflow models is the concept of actors, which encapsulate computation and which have \emph{firing} semantics.\index{firing}
Actors communicate exclusively via explicit input and output channels, which work as \acs{FIFO} buffers.
An actor fires when certain conditions are met, consuming tokens in (some of) its input channels, and producing other tokens in its output channels.

We will describe Dennis dataflow using a formalism similar to the one described in~\cite{Parks:M95:105,lee_matsikoudis_semantics}.
This formalism is very general and allows to describe many other dataflow paradigms as special cases.
The basis of the formalism are the \emph{firirng rules}\index{firing rules}.
An actor has a finite set $\mathcal{R}$ of firing rules, and each rule $R \in \mathcal{R}$ is a finite tuple of words over the alphabet of values $\bar \Sigma := \Sigma \cup \{\bot\}$.
Here, $\bot$ represents an \emph{abscent} value\index{absent value}, which means no data has to be present in that channel for the actor to fire.
%In the literature\footnote{}, the formulation it is not clear about whether $\mathcal{R}$ has to be finite, but we will demand it is for simplicity, since we don't see any practical value in the case where it is truly infinite.
The patterns are sometimes also interpreted to be words in an extended alphabet with wildcards, e.g. $\Sigma \cup \{\bot,*\}$, where $*$ stands for any value in $\Sigma$.
Note that, mathematically speaking, both $\bot$ and $*$ are unnecessary, as the empty string $\epsilon$ has the same effect as $\bot$ and $*$ can be replaced by a series of rules, one for each value in $\Sigma$.
In most pratical instances of dataflow, on the other hand, rules only consist of values in $\{\bot,*\}$, which is why they are very useful for descriptions.

An actor fires whenever there is enough tokens in the input channels to satisfy a rule.
Here, satisfying a rule specifically means the rule $R$ is a prefix of the channel values $C$, i.e. $R \sqsubseteq C$.
If we include special values $\bot$ and $*$, the pattern has to be interpreted, e.g. by transforming it into the mathematically equivalent variants explained above.
In this case, the tokens are consumed from the channels and the actor executes, computing something and potentially producing some outputs, which are not part of the specification in the firing rules.

Note that there is nothing preventing multiple rules to apply simultaneously. For example, an actor with two inputs could have the rules $(*,\bot)$ and $(\bot,*)$, firing as soon as one of the two channels has a token.
If multiple rules apply simultaneously, there is no general order in which the actor fires and consumes the inputs. 
This means that this model is non-deterministic.
We denote this very general, dynamic variant as \ac{DDF} (alternatively, Dennis Data Flow).\index{\ac{DDF}}

If we add an additional condition, requiring that for two rules $R,R'$ there is no upper bound $S$ (i.e. with $R \sqsubseteq S, R' \sqsubseteq S$), then we can show that the model is deterministic.
We can even relax this condition somewhat and keep determinism. In~\cite{lee_matsikoudis_semantics}, the authors show this by explicitly constructing a Scott-continuous function from actor firings and embedding the model into \ac{KPN}.
They also discuss possible relaxations.
This deterministic variant of (Dennis) Dataflow is sometimes referred to as \ac{DPN}.\index{\ac{DPN}}

All these models are very expressive, so much so that they do not permit very strong analysis of their behavior.
In contrast, the \ac{SDF} model\index{Dataflow ! SDF}~\cite{lee1987sdf} has a very well-defined behavior and allows more analysis to be done statically, like scheduling or bounding the sizes of the channels~\cite{Parks:M95:105}
The firing rates in the \ac{SDF} model are fixed. In the formalism, this means the firing rules are always of the form $(*^{n_1},\ldots,*^{n_k})$, where $*^0 = \epsilon \mathrel{\widehat{=}} \bot$.
Moreover, the number of tokens \emph{produced} is also fixed statically, which is not part of the formalism of firing explained above. 
An apparently more strict variant of \ac{SDF} is \ac{HSDF}, in which all the rates are $1$. However, these two are equivalently expressive: every \ac{SDF} graph can be unrolled to an equivalent \ac{HSDF} graph.
The semantics of \ac{HSDF} are basically equivalent with the widespread model of \emph{task graphs}\index{task graphs}, which are widespread in the design of embedded systems and \ac{HLS}.


We discuss two additional variants of dataflow which sit semantically between \ac{SDF} and \ac{DDF}. 
The first is \ac{CSDF}\index{\ac{CSDF}}~\cite{bilsen1996cycle}, in which the static values of \ac{SDF} are replaced with cycles that repeat, allowing for some controlled dynamism while retaining the analysability.
Finally, \ac{SADF}\index{\ac{SADF}}~\cite{theelen2006scenario} is a more general model which allows to enable and disable certain paths in the graph, which are otherwise static.

Figure~\ref{fig:dataflow_mocs} shows a Venn diagram of the dataflow models discussed here and their relationship.
Here we draw the distinctions as strict as possible.
For example, we draw \ac{HSDF} as a subset of \ac{SDF} since, definitionally, it is, even though they have the same semantic expressive power.
In other words, every \ac{HSDF} is an \ac{SDF}, and conversely, not every \ac{SDF} is an \ac{HSDF}, even though there exists an equivalent (unrolled) \ac{HSDF}, it is just equivalent, not identical.
We also include \ac{KPN} and the \ac{KMQ} blocking-reads semantics since they are commonly discussed as dataflow models as well.
Since the models are fundamentally different, we have depict them in the Venn diagram as what is embeddable semantically.
Note that we depict \ac{DPN} as being included in \ac{KMQ} (which is proven in~\cite{lee_matsikoudis_semantics}), but we do not know if this inclusion is strict, in other words, if there are \ac{KMQ} models which are not expressable as \ac{DPN}.
We will discuss the difference between \ac{KMQ} and \ac{KPN} in Section~\ref{sec:macqueen}, where we also show that this inclusion is strict.

\begin{figure}[h]
	\centering
   \resizebox{0.65\textwidth}{!}{\input{figures/dataflow_mocs.tex}}
	\caption{Relationships between different dataflow models of computation.}
	\label{fig:dataflow_mocs}
\end{figure}